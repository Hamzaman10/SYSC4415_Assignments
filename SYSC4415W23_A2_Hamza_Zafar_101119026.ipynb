{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamzaman10/SYSC4906_Assignments/blob/main/SYSC4415W23_A2_Hamza_Zafar_101119026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SYSC4415 - Assignment 2\n",
        "\n",
        "**TA: Fran√ßois Charih \\<francois@charih.ca\\>**\n",
        "\n",
        "**Deadline: March 19th, 2023 @ 11:59PM**\n",
        "\n",
        "## Learning objectives\n",
        "\n",
        "1. Fine-tune pre-trained CNN architectures for a custom image classification challenge.\n",
        "\n",
        "2. Evaluate the performance of machine learning models using different metrics (precision-recall curve, confusion matrices, *etc.*).\n",
        "\n",
        "3. Get hands-on experience with modern machine learning and plotting libraries.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. Give yourself plenty of time to complete the assignment (it could take you up to 8-10 hours if you are unfamiliar with Python and machine learning libraries). The models should not take more than 1h to fine-tune (my full notebook runs from start to end in a little under 40 minutes. Coding will be the most time-consuming. ‚ö†Ô∏è**Do not wait to the last minute.** ‚ö†Ô∏è\n",
        "\n",
        "2. You must use the prescribed methods/functions/libraries mentioned, whenever specified. The functions you need are already imported for you in the appropriate sections. You can reorganize the imports and import the full packages instead of specific functions if you so desire. I imported the necessary function for you so that they are ready to be invoked without have to specify the full path to the functions with the dot operator (i.e. so that you invoke `function()` instead of `package.module.function()`).\n",
        "\n",
        "3. Make sure to include comments for non-trivial code. It is okay to add some code cells, if you think it will give your code better readability/structure.\n",
        "\n",
        "4. If you are unsure about something, clearly state your assumptions and complete the question based off these assumptions.\n",
        "\n",
        "5. Be careful as you complete the assignment. There are several text-based questions to be answered in Markdown (text) cells. The questions are accompanied by the ‚ùì emoji. Your answers should be entered in the markdown cells with the üìù emoji.\n",
        "\n",
        "6. Submit your Notebook as both a `.ipynb` file that adopts this naming convention: *SYSC4415W23_A2_\\<First Name\\>_\\<Last Name\\>_\\<StudentID\\>.ipynb* on Brightspace. I should be able to run your code without errors.\n",
        "\n",
        "7. Make sure you enable a GPU accelerator (in Runtime > Change runtime type) starting at Part 4 and that your training code uses it. GPU resources are limited, so it is recommended not to use the accelerator for prior steps.\n",
        "\n",
        "8. All plots should be made with matplotlib and labeled properly (ie. include axis labels and legends).\n",
        "\n",
        "## Context\n",
        "\n",
        "It is 2030, and a new RNA virus named SARS-CoV-3 is wreaking havoc across the globe. Its death rate is estimated at 95%, making it one of the deadliest  respiratory viruses known to mankind. Fortunately, an Ottawa-based biotech company developed a nasally-delivered vaccine *Greenraza*‚Ñ¢Ô∏è that can neutralize the virus in living patients. However, administrating the vaccine increases the risk of lung cancer by a whooping 60%. It is therefore vital that the drug be administered to infected patients only, not to patients infected with another respiratory virus such as the common cold or influenza. The virus cannot be detected through blood or breath analyses. It can only be detected by means of x-ray imaging.\n",
        "\n",
        "Having heard of your newly developed expertise in deep learning, you have been tasked by the Ottawa Hospital with the design of a machine learning model capable of distinguishing patients infected with SARS-CoV-3 from patients that have pneumonia and non-infected patients. Healthy patients can be discharged, while patients with pneumonia must be isolated, but without being given *Greenraza*‚Ñ¢Ô∏è."
      ],
      "metadata": {
        "id": "PGuAmMpysXXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project initialization\n",
        "\n",
        "Run the cells below to set-up the notebook (ie. download the dataset) and install the required external libraries."
      ],
      "metadata": {
        "id": "c33I6cJzIn0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS (downloads the dataset)\n",
        "! rm -rf SYSC4415W23_A2_dataset SYSC4415W23_A2_dataset.zip\n",
        "! wget https://github.com/jrgreen7/SYSC4906/releases/download/Assignment2/SYSC4415W23_A2_dataset.zip && unzip SYSC4415W23_A2_dataset.zip"
      ],
      "metadata": {
        "id": "TEbQzpX8j7bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS (installs external libraries)\n",
        "!pip install timm\n",
        "!pip install git+https://github.com/nottombrown/imagenet_stubs\n",
        "!pip install torchstat"
      ],
      "metadata": {
        "id": "NanqafBgkZFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TJdiBb1oSLiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Preparing the dataset\n",
        "\n",
        "The Ottawa Hospital has provided you with a dataset to develop your model. The dataset is available here. The dataset contains a folder containing a spreadsheet with metadata for each image in the dataset and a subfolder containing the 200x200 images (with random filenames)."
      ],
      "metadata": {
        "id": "q1XKrzDrs11o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "from matplotlib.pyplot import bar, xlabel, ylabel, title"
      ],
      "metadata": {
        "id": "cLf1gprJKQ1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Using pandas' `read_csv` function, load the dataframe containing the image metadata (`dataset_metadata.csv`)."
      ],
      "metadata": {
        "id": "QHTpQfihKSsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the image metadata into a pandas dataframe\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "metadata = read_csv(\"/content/SYSC4415W23_A2_dataset/dataset_metadata.csv\")"
      ],
      "metadata": {
        "id": "1qhfruYusvHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Use the `head` method to print the top five rows of the dataframe."
      ],
      "metadata": {
        "id": "btf3Q-VK_o1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints the top five rows in the dataset\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(metadata.head())"
      ],
      "metadata": {
        "id": "SkWvyTpjn8pE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe18869b-42a7-49e2-92db-9b28994e08d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          filename     split            xray_date  height  width   label\n",
            "0   272_normal.jpg  training  2030-11-18 06:50:42     200    200  normal\n",
            "1   788_normal.jpg  training  2030-06-20 21:32:45     200    200  normal\n",
            "2   622_normal.jpg  training  2030-07-12 06:56:19     200    200  normal\n",
            "3  1138_normal.jpg  training  2030-03-15 13:52:16     200    200  normal\n",
            "4  1568_normal.jpg  training  2030-12-22 22:09:18     200    200  normal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Using pandas' [query method](https://pandas.pydata.org/docs/user_guide/indexing.html#the-query-method) and the `len` methods on the selections, print the number of images in the training, validation and test sets."
      ],
      "metadata": {
        "id": "n-ZA-MdI_zzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints the dimensions of the dataframe\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"the number of images in the training set is:\", len(metadata.query('split == \"training\"')))\n",
        "print(\"the number of images in the test set is:\", len(metadata.query('split == \"test\"')))\n",
        "print(\"the number of images in the validation set is:\", len(metadata.query('split == \"validation\"')))"
      ],
      "metadata": {
        "id": "ewWdw9Zvx5x6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d18c1e21-54b3-4c27-d805-3fb1a20318b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of images in the training set is: 2222\n",
            "the number of images in the test set is: 505\n",
            "the number of images in the validation set is: 645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4:** Using the `value_counts` method on the ‚Äúlabel‚Äù column, provide the composition of the dataset in terms of the number of SARS-CoV-3 cases, pneumonia cases and healthy x-rays. In another cell, prepare a bar chart from that data using matplotlib's `bar` method. Note that the result of the `value_counts` methods is a series object whose property `index` is the label."
      ],
      "metadata": {
        "id": "D_cEMcdnwzpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates a breakdown of the images' classes\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#sarscov3 pneumonia\n",
        "\n",
        "print(metadata['label'].value_counts())"
      ],
      "metadata": {
        "id": "XnNdC4_XxOV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f762ab94-ee17-45a8-ea06-3b86f8633b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normal       1500\n",
            "pneumonia    1300\n",
            "sarscov3      572\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots a bar chart\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "query = (metadata['label'].value_counts())\n",
        "bar(range(len(query)), query)\n",
        "x = xlabel(\"virus\")\n",
        "y = ylabel(\"number of cases\")\n",
        "title = (\"Test\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DgEUaXfujlR3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "3b352026-9db5-4764-9fee-08f1acbdf83a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEGCAYAAACdJRn3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXl0lEQVR4nO3df9RdVX3n8ffH8EtbR35FpIQYqJlO0dGKGaSjq7XSpaBW6FQROksComkVfw1OFXUtcWxdC8aOVtqOmgo1uig/irZkEHUYwHEchTEgvxlqyg9JFkgUCCqKBr7zx91xLiFPcs7zPPfe50ner7XOuufss+85380J+Waffe4+qSokSerjSZMOQJI0/5g8JEm9mTwkSb2ZPCRJvZk8JEm97TLpAEZh3333rSVLlkw6DEmaV6655prvV9XCLnV3yOSxZMkS1qxZM+kwJGleSXJX17retpIk9WbykCT1ZvKQJPVm8pAk9WbykCT1ZvKQJPVm8pAk9WbykCT1ZvKQJPW2Q/7CfKaWnPbFSYeww7rzjFdOOgRJs8CehySpN5OHJKk3k4ckqTeThySpN5OHJKk3k4ckqTeThySpN5OHJKk3k4ckqbeRJY8k5yS5L8lNW9n3riSVZN+2nSRnJVmb5IYkhw7VXZ7kO21ZPqp4JUndjbLn8RngyC0LkxwIvAz47lDxUcDStqwAPtHq7g2cDrwQOAw4PcleI4xZktTByJJHVX0NuH8ruz4GvBuoobKjgc/WwFXAnkn2B14OXFZV91fVA8BlbCUhSZLGa6xjHkmOBtZX1fVb7DoAuHtoe10rm6pckjRBY5tVN8lTgPcxuGU1iuOvYHDLi8WLF4/iFJKkZpw9j18FDgKuT3InsAi4NskzgPXAgUN1F7WyqcqfoKpWVtWyqlq2cOHCEYQvSdpsbMmjqm6sqqdX1ZKqWsLgFtShVXUvsBo4oT11dTiwsaruAb4CvCzJXm2g/GWtTJI0QaN8VPc84JvAryVZl+TkbVS/FLgdWAv8DfAWgKq6H/hT4Ftt+VArkyRN0MjGPKrq+O3sXzK0XsApU9Q7BzhnVoOTJM2IvzCXJPVm8pAk9WbykCT1ZvKQJPVm8pAk9WbykCT1ZvKQJPVm8pAk9Ta2iRGlUVly2hcnHcIO684zXjnpEDRH2fOQJPVm8pAk9WbykCT1ZvKQJPVm8pAk9WbykCT1ZvKQJPVm8pAk9WbykCT1ZvKQJPU2suSR5Jwk9yW5aajsI0n+b5IbkvxDkj2H9r03ydoktyV5+VD5ka1sbZLTRhWvJKm7UfY8PgMcuUXZZcBzquq5wD8B7wVIcghwHPDs9p3/mmRBkgXAXwNHAYcAx7e6kqQJGlnyqKqvAfdvUfbfq2pT27wKWNTWjwbOr6pHquoOYC1wWFvWVtXtVfUz4PxWV5I0QZMc83gD8KW2fgBw99C+da1sqvInSLIiyZokazZs2DCCcCVJm00keSR5P7AJOHe2jllVK6tqWVUtW7hw4WwdVpK0FWN/n0eSE4FXAUdUVbXi9cCBQ9UWtTK2US5JmpCx9jySHAm8G3h1VT08tGs1cFyS3ZMcBCwF/g/wLWBpkoOS7MZgUH31OGOWJD3RyHoeSc4DXgLsm2QdcDqDp6t2By5LAnBVVf1xVd2c5ELgFga3s06pqkfbcd4KfAVYAJxTVTePKmZJUjcjSx5VdfxWis/eRv0PAx/eSvmlwKWzGJokaYb8hbkkqTeThySpN5OHJKk3k4ckqTeThySpN5OHJKk3k4ckqTeThySpN5OHJKk3k4ckqTeThySpt+0mjyS/mmT3tv6SJG8ffve4JGnn06Xn8Xng0STPAlYyeL/G3400KknSnNYleTzW3jv++8BfVtWfAPuPNixJ0lzWJXn8PMnxwHLgkla26+hCkiTNdV2Sx0nAbwIfrqo72pv+PjfasCRJc9l2XwZVVbckeQ+wuG3fAZw56sAkSXNXl6etfg+4Dvhy2/6NJL5HXJJ2Yl1uW30QOAx4EKCqrgMOHllEkqQ5r9OAeVVt3KLsse19Kck5Se5LctNQ2d5JLkvynfa5VytPkrOSrE1yQ5JDh76zvNX/TpLlXRsmSRqdLsnj5iR/CCxIsjTJXwLf6PC9zwBHblF2GnB5VS0FLm/bAEcBS9uyAvgEDJINcDrwQga9n9M3JxxJ0uR0SR5vA54NPAKcBzwEvHN7X6qqrwH3b1F8NLCqra8Cjhkq/2wNXAXsmWR/4OXAZVV1f1U9AFzGExOSJGnMujxt9TDwfuD9SRYAv1RVP53m+farqnva+r3Afm39AODuoXrrWtlU5U+QZAWDXguLFy+eZniSpC66PG31d0n+RZJfAm4EbknyJzM9cVUVUDM9ztDxVlbVsqpatnDhwtk6rCRpK7rctjqkqh5icIvpS8BBwOuneb7vtdtRtM/7Wvl6BnNmbbaolU1VLkmaoC7JY9ckuzJIHqur6udMv8ewmsE0J7TPi4fKT2hPXR0ObGy3t74CvCzJXm2g/GWtTJI0Qdsd8wA+BdwJXA98LckzGQyab1OS84CXAPsmWcfgqakzgAuTnAzcBRzbql8KvAJYCzzMYEoUqur+JH8KfKvV+1BVbTkIL0kasy4D5mcBZw0V3ZXkdzp87/gpdh2xlboFnDLFcc4Bztne+SRJ49Ol50GSVzJ4XHePoeIPjSQiSdKc1+Vpq08Cr2Pwe48ArwWeOeK4JElzWJcB839bVScAD1TVf2IwPfu/HG1YkqS5rEvy+En7fDjJrwA/xzcJStJOrcuYxyVJ9gQ+AlzL4DHdT48yKEnS3Nblaas/baufT3IJsMdWZtmVJO1EugyYn9J6HlTVI8CTkrxl1IFJkuauLmMeb6qqBzdvtNlt3zSyiCRJc16X5LEgSTZvtJl1dxtdSJKkua7LgPmXgQuSfKpt/1ErkyTtpLokj/cweE/Gm9v2Zfi0lSTt1Lo8bfUY8Mm2SJLUacxDkqTHMXlIknqbMnkk+Vz7fMf4wpEkzQfb6nm8oM1l9Yb2Jr+9h5dxBShJmnu2NWD+SeBy4GDgGgbTsW9WrVyStBOasudRVWdV1a8D51TVwVV10NBi4pCkndh2B8yr6s1JnpfkrW157kxPmuQ/JLk5yU1JzkuyR5KDklydZG2SC5Ls1uru3rbXtv1LZnp+SdLMdJkY8e3AucDT23JukrdN94RJDgDeDiyrqucAC4DjgDOBj1XVs4AHgJPbV05m8CKqZwEfa/UkSRPU5VHdNwIvrKoPVNUHgMOZ+cSIuwBPTrIL8BTgHuClwEVt/yrgmLZ+dNum7T9ieK4tSdL4dUkeAR4d2n6Uxw+e91JV64E/B77LIGlsZDAg/2BVbWrV1gEHtPUDgLvbdze1+vs8IchkRZI1SdZs2LBhuuFJkjroMrfV3wJXJ/mHtn0McPZ0T5hkLwa9iYOAB4G/B46c7vE2q6qVwEqAZcuW1UyPJ0maWpe5rT6a5KvAi1vRSVX17Rmc83eBO6pqA0CSLwAvAvZMskvrXSwC1rf664EDgXXtNtfTgB/M4PySpBnq0vOgqq5l8P7y2fBd4PAkTwF+AhwBrAGuBF4DnA8sBy5u9Ve37W+2/VdUlT0LSZqgsc9tVVVXMxj4vha4scWwksHU76cmWctgTGPzrbGzgX1a+anAaeOOWZL0eJ16HrOtqk4HTt+i+HbgsK3U/Snw2nHEJUnqZps9jyQLklw5rmAkSfPDNpNHVT0KPJbkaWOKR5I0D3S5bfUj4MYklwE/3lxYVW8fWVSSpDmtS/L4QlskSQK6/c5jVZInA4ur6rYxxCRJmuO6TIz4e8B1wJfb9m8kWT3iuCRJc1iX33l8kMEjtA8CVNV1+CIoSdqpdUkeP6+qjVuUPTaKYCRJ80OXAfObk/whsCDJUgbv4vjGaMOSJM1lXXoebwOeDTwCnAc8BLxzhDFJkua4Lk9bPQy8P8mZg8364ejDkiTNZV2etvo3SW4EbmDwY8Hrk7xg9KFJkuaqLmMeZwNvqar/BZDkxQxeEPXcUQYmSZq7uox5PLo5cQBU1deBTduoL0nawU3Z80hyaFv9n0k+xWCwvIDXAV8dfWiSpLlqW7et/ssW28Pv3/BNfpK0E5syeVTV74wzEEnS/LHdAfMkewInAEuG6zsluyTtvLo8bXUpcBWD9407LYkkqVPy2KOqTp3Nk7bezKeB5zAYP3kDcBtwAYMezp3AsVX1QJIAHwdeATwMnFhV185mPJKkfro8qvu5JG9Ksn+SvTcvMzzvx4EvV9W/Ap4H3AqcBlxeVUuBy9s2wFHA0rasAD4xw3NLkmaoS/L4GfAR4JvANW1ZM90Ttveh/xaDHx9SVT+rqgeBo4FVrdoq4Ji2fjTw2Rq4Ctgzyf7TPb8kaea63LZ6F/Csqvr+LJ3zIGAD8LdJnscgGb0D2K+q7ml17gX2a+sHAHcPfX9dK7tnqIwkKxj0TFi8ePEshSpJ2pouPY+1DMYaZssuwKHAJ6rq+cCP+f+3qIDB7Iv0/C1JVa2sqmVVtWzhwoWzFqwk6Ym69Dx+DFyX5EoG07IDM3pUdx2wrqqubtsXMUge30uyf1Xd025L3df2rwcOHPr+olYmSZqQLsnjH9syK6rq3iR3J/m1qroNOAK4pS3LgTPa58XtK6uBtyY5H3ghsHHo9pYkaQK6vM9j1fbqTMPbgHOT7AbcDpzE4BbahUlOBu4Cjm11L2XwmO7m22cnjSAeSWO05LQvTjqEHdadZ7xyLOfp8gvzO9jK+ENVHTzdk1bVdcCyrew6Yit1CzhluueSJM2+Lrethv+S3wN4LTDT33lIkuax7T5tVVU/GFrWV9VfAOPpF0mS5qQut60OHdp8EoOeSJceiyRpB9UlCQy/12MTbd6pkUQjSZoXujxt5Xs9JEmP0+W21e7AH/DE93l8aHRhSZLmsi63rS4GNjKYg+qR7dSVJO0EuiSPRVV15MgjkSTNG10mRvxGkn898kgkSfNGl57Hi4ET2y/NHwHC4Iffzx1pZJKkOatL8jhq5FFIkuaVLo/q3jWOQCRJ80eXMQ9Jkh7H5CFJ6s3kIUnqzeQhSerN5CFJ6s3kIUnqbWLJI8mCJN9OcknbPijJ1UnWJrmgvd+cJLu37bVt/5JJxSxJGphkz+MdwK1D22cCH6uqZwEPACe38pOBB1r5x1o9SdIETSR5JFnE4FW2n27bAV4KXNSqrAKOaetHt23a/iNafUnShEyq5/EXwLuBx9r2PsCDVbWpba8DDmjrBwB3A7T9G1t9SdKEjD15JHkVcF9VXTPLx12RZE2SNRs2bJjNQ0uStjCJnseLgFcnuRM4n8Htqo8DeybZPNfWImB9W18PHAjQ9j8N+MGWB62qlVW1rKqWLVy4cLQtkKSd3NiTR1W9t6oWVdUS4Djgiqr698CVwGtateUM3mAIsLpt0/ZfUVU1xpAlSVuYS7/zeA9wapK1DMY0zm7lZwP7tPJTgdMmFJ8kqenyPo+RqaqvAl9t67cDh22lzk+B1441MEnSNs2lnockaZ4weUiSejN5SJJ6M3lIknozeUiSejN5SJJ6M3lIknozeUiSejN5SJJ6M3lIknozeUiSejN5SJJ6M3lIknozeUiSejN5SJJ6M3lIknozeUiSejN5SJJ6M3lIknobe/JIcmCSK5PckuTmJO9o5XsnuSzJd9rnXq08Sc5KsjbJDUkOHXfMkqTHm0TPYxPwrqo6BDgcOCXJIcBpwOVVtRS4vG0DHAUsbcsK4BPjD1mSNGzsyaOq7qmqa9v6D4FbgQOAo4FVrdoq4Ji2fjTw2Rq4Ctgzyf7jjVqSNGyiYx5JlgDPB64G9quqe9que4H92voBwN1DX1vXyrY81ooka5Ks2bBhw+iCliRNLnkk+WXg88A7q+qh4X1VVUD1OV5VrayqZVW1bOHChbMYqSRpSxNJHkl2ZZA4zq2qL7Ti722+HdU+72vl64EDh76+qJVJkiZkEk9bBTgbuLWqPjq0azWwvK0vBy4eKj+hPXV1OLBx6PaWJGkCdpnAOV8EvB64Mcl1rex9wBnAhUlOBu4Cjm37LgVeAawFHgZOGmu0kqQnGHvyqKqvA5li9xFbqV/AKSMNSpLUi78wlyT1ZvKQJPVm8pAk9WbykCT1ZvKQJPVm8pAk9WbykCT1ZvKQJPVm8pAk9WbykCT1ZvKQJPVm8pAk9WbykCT1ZvKQJPVm8pAk9WbykCT1ZvKQJPVm8pAk9WbykCT1Nm+SR5Ijk9yWZG2S0yYdjyTtzOZF8kiyAPhr4CjgEOD4JIdMNipJ2nnNi+QBHAasrarbq+pnwPnA0ROOSZJ2WrtMOoCODgDuHtpeB7xwuEKSFcCKtvmjJLdtcYx9ge+PLMLJmVftypmdq86rdvU0b9rW43rBPGrXNMybts3wmj2z6xfnS/LYrqpaCaycan+SNVW1bIwhjYXtmn921LbtqO2CHbdtM2nXfLlttR44cGh7USuTJE3AfEke3wKWJjkoyW7AccDqCcckSTuteXHbqqo2JXkr8BVgAXBOVd3c8zBT3tKa52zX/LOjtm1HbRfsuG2bdrtSVbMZiCRpJzBfbltJkuYQk4ckqbcdNnkk2TvJZUm+0z73mqLeo0mua8ucHYTf3vQsSXZPckHbf3WSJRMIs7cO7ToxyYaha/TGScTZV5JzktyX5KYp9ifJWa3dNyQ5dNwxTkeHdr0kycah6/WBccc4HUkOTHJlkluS3JzkHVupM1+vWZe29b9uVbVDLsB/Bk5r66cBZ05R70eTjrVDWxYA/wwcDOwGXA8cskWdtwCfbOvHARdMOu5ZateJwF9NOtZptO23gEOBm6bY/wrgS0CAw4GrJx3zLLXrJcAlk45zGu3aHzi0rT8V+Ket/Fmcr9esS9t6X7cdtufBYPqSVW19FXDM5EKZsS7Tswy39yLgiCQZY4zTscNOO1NVXwPu30aVo4HP1sBVwJ5J9h9PdNPXoV3zUlXdU1XXtvUfArcymNli2Hy9Zl3a1tuOnDz2q6p72vq9wH5T1NsjyZokVyU5Zjyh9ba16Vm2vPi/qFNVm4CNwD5jiW76urQL4A/abYKLkhy4lf3zUde2z0e/meT6JF9K8uxJB9NXu+X7fODqLXbN+2u2jbZBz+s2L37nMZUk/wN4xlZ2vX94o6oqyVTPJD+zqtYnORi4IsmNVfXPsx2rpu2/AedV1SNJ/ohB7+qlE45JU7uWwf9TP0ryCuAfgaWTDam7JL8MfB54Z1U9NOl4ZtN22tb7us3rnkdV/W5VPWcry8XA9zZ3KdvnfVMcY337vB34KoOsPNd0mZ7lF3WS7AI8DfjBWKKbvu22q6p+UFWPtM1PAy8YU2yjtkNOuVNVD1XVj9r6pcCuSfadcFidJNmVwV+u51bVF7ZSZd5es+21bTrXbV4nj+1YDSxv68uBi7eskGSvJLu39X2BFwG3jC3C7rpMzzLc3tcAV1QbCZvDttuuLe4pv5rB/dodwWrghPYEz+HAxqHbrPNWkmdsHmtLchiDv2Pm+j9iaDGfDdxaVR+dotq8vGZd2jad6zavb1ttxxnAhUlOBu4CjgVIsgz446p6I/DrwKeSPMbgP9YZVTXnkkdNMT1Lkg8Ba6pqNYM/HJ9LspbBgOZxk4u4m47tenuSVwObGLTrxIkF3EOS8xg8wbJvknXA6cCuAFX1SeBSBk/vrAUeBk6aTKT9dGjXa4A3J9kE/AQ4bh78IwYG/3B8PXBjkuta2fuAxTC/rxnd2tb7ujk9iSSptx35tpUkaURMHpKk3kwekqTeTB6SpN5MHpKk3kwe0ggk+ZUkF006DmlUfFRXGqMku7S5x6R5zZ6HNENJzkhyytD2B5P8x83vvMjgnSSrk1wBXN7enXDJUP2/SnLi0LFuaRNB/vm42yJ1ZfKQZu4C2gwGzbE8cdbSQ4HXVNVvT3WQJPsAvw88u6qeC/zZbAcqzRaThzRDVfVt4OltnON5wAM8fupugMuqanvvwdgI/BQ4O8m/YzAFhjQnmTyk2fH3DOYHeh2DnsiWfjy0vonH/7+3B/ziPSyHMXiZ16uAL48kUmkW7MgTI0rjdAHwN8C+wG8Du2+j7l3AIW1G5ycDRwBfb+9beEpVXZrkfwO3jzhmadpMHtIsaLMBPxVYX1X3tDe2TVX37iQXAjcBdwDfbrueClycZA8G78k+dcRhS9Pmo7qSpN4c85Ak9WbykCT1ZvKQJPVm8pAk9WbykCT1ZvKQJPVm8pAk9fb/AHdwJcfe5N2nAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùì ***Question: Is the data balanced or not? If not, specify why class imbalance makes classification more difficult, and suggest one method you could use to deal with the imbalanced data.***\n",
        "\n",
        "Yes this dataset is imbalanced, as we can see there's significantly more normal and pneumonia cases than saars-cov. This makes classification more difficult because it can cause the model to be biased towards normal and pneumonia. One method to deal with this is oversampling. With oversampling yo ucan duplicate the data in the minority class to increase the number of it's examples"
      ],
      "metadata": {
        "id": "7actPHfExeYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù YOUR ANSWER GOES HERE\n",
        "\n"
      ],
      "metadata": {
        "id": "6QR5HTn_m9P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Applying pre-trained CNN models to the data\n",
        "\n",
        "Researchers make pre-trained neural networks available to the community at large. There are many, many pre-trained CNNs available in online repositories that researchers can leverage for their own applications."
      ],
      "metadata": {
        "id": "2gGFYwdHBWvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.io import read_image\n",
        "from timm import create_model\n",
        "from matplotlib.pyplot import imread, imshow\n",
        "from imagenet_stubs.imagenet_2012_labels import label_to_name\n",
        "import torch"
      ],
      "metadata": {
        "id": "vNx-cQJlKCi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Display the image `SYSC4415W23_A2_dataset/training/sarscov3/100_sarscov3.jpg` from the test set. The matplotlib methods `imread` and `imshow` are useful. üòâ"
      ],
      "metadata": {
        "id": "fxQ7F3K6KCvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the image with matplotlib's imread/imshow\n",
        "\n",
        "# YOUR CODE HERE\n",
        "display = imread('SYSC4415W23_A2_dataset/training/sarscov3/100_sarscov3.jpg')\n",
        "imshow(display)"
      ],
      "metadata": {
        "id": "ckGQkYL6k41-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "eb5e3e4c-ac82-4f5d-9b6c-c618f59784d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff901004550>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Load the ImageNet-pretrained InceptionV3 (`inception_v3`) and ResNet50 (`resnet50`) models with `timm` (this was done in Anthony's CIFAR100 tutorial with PyTorch). Use them to predict the class of the image along with the probability (not the logit). The probability is the result of applying the softmax function to the logits.\n",
        "\n",
        "Of course, because the models were pretrained on ImageNet, it will not predict any of the classes that interest us.\n",
        "\n",
        "***Notes:**\n",
        "1. The `read_image` function can convert an image on disk to a tensor.\n",
        "\n",
        "2. The function `label_to_name` that I imported for you converts the index of an ImageNet class to its English name."
      ],
      "metadata": {
        "id": "42hsfWsyBDPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the image in a Tensor (the method read_image can load the image in a tensor), calls unsqueeze(0) on the tensor to add a dimension and convert its entries to floats using the .float() method\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "#read image\n",
        "img = read_image('SYSC4415W23_A2_dataset/training/sarscov3/100_sarscov3.jpg')\n",
        "img = img.unsqueeze(0)\n",
        "img = img.float()\n",
        "print(img)"
      ],
      "metadata": {
        "id": "yJ49xCUJl_ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda8d47c-fd36-42b0-e661-b516b53d6ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 10.,  10.,  10.,  ...,   7.,   7.,   7.],\n",
            "          [ 10.,  10.,  10.,  ...,   8.,   8.,   7.],\n",
            "          [ 10.,  10.,  10.,  ...,   6.,   7.,   7.],\n",
            "          ...,\n",
            "          [  9.,   8.,   6.,  ..., 121., 115., 109.],\n",
            "          [  4.,   8.,   8.,  ..., 121., 115., 109.],\n",
            "          [  8.,   7.,   3.,  ..., 119., 112., 105.]],\n",
            "\n",
            "         [[ 10.,  10.,  10.,  ...,   7.,   7.,   7.],\n",
            "          [ 10.,  10.,  10.,  ...,   8.,   8.,   7.],\n",
            "          [ 10.,  10.,  10.,  ...,   6.,   7.,   7.],\n",
            "          ...,\n",
            "          [  9.,   8.,   6.,  ..., 121., 115., 109.],\n",
            "          [  4.,   8.,   8.,  ..., 121., 115., 109.],\n",
            "          [  8.,   7.,   3.,  ..., 119., 112., 105.]],\n",
            "\n",
            "         [[ 10.,  10.,  10.,  ...,   7.,   7.,   7.],\n",
            "          [ 10.,  10.,  10.,  ...,   8.,   8.,   7.],\n",
            "          [ 10.,  10.,  10.,  ...,   6.,   7.,   7.],\n",
            "          ...,\n",
            "          [  9.,   8.,   6.,  ..., 121., 115., 109.],\n",
            "          [  4.,   8.,   8.,  ..., 121., 115., 109.],\n",
            "          [  8.,   7.,   3.,  ..., 119., 112., 105.]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the InceptionV3 model predicted class along with its probability\n",
        "\n",
        "# YOUR CODE HERE\n",
        "inceptionV3Model = create_model('inception_v3', pretrained=True)\n",
        "output = inceptionV3Model(img)\n",
        "softmax = output.softmax(1)\n",
        "print(softmax)"
      ],
      "metadata": {
        "id": "Qy3maVQ9CJag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d01d59-4fcb-4bdc-ed4a-349c22f3c68a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0009, 0.0010, 0.0009, 0.0008, 0.0012, 0.0009, 0.0008, 0.0010, 0.0009,\n",
            "         0.0009, 0.0009, 0.0009, 0.0008, 0.0009, 0.0008, 0.0010, 0.0009, 0.0009,\n",
            "         0.0011, 0.0008, 0.0011, 0.0010, 0.0008, 0.0011, 0.0009, 0.0010, 0.0012,\n",
            "         0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0011, 0.0009, 0.0009, 0.0008,\n",
            "         0.0010, 0.0009, 0.0011, 0.0009, 0.0009, 0.0008, 0.0010, 0.0009, 0.0009,\n",
            "         0.0010, 0.0009, 0.0009, 0.0009, 0.0010, 0.0010, 0.0012, 0.0010, 0.0009,\n",
            "         0.0011, 0.0010, 0.0009, 0.0009, 0.0008, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "         0.0009, 0.0008, 0.0009, 0.0010, 0.0010, 0.0008, 0.0009, 0.0008, 0.0009,\n",
            "         0.0008, 0.0008, 0.0009, 0.0008, 0.0010, 0.0009, 0.0009, 0.0010, 0.0010,\n",
            "         0.0010, 0.0011, 0.0009, 0.0009, 0.0009, 0.0011, 0.0009, 0.0010, 0.0009,\n",
            "         0.0009, 0.0009, 0.0009, 0.0011, 0.0009, 0.0009, 0.0010, 0.0009, 0.0010,\n",
            "         0.0010, 0.0008, 0.0009, 0.0009, 0.0011, 0.0011, 0.0009, 0.0010, 0.0007,\n",
            "         0.0009, 0.0009, 0.0008, 0.0010, 0.0011, 0.0010, 0.0011, 0.0007, 0.0012,\n",
            "         0.0010, 0.0008, 0.0010, 0.0008, 0.0010, 0.0011, 0.0009, 0.0010, 0.0010,\n",
            "         0.0009, 0.0009, 0.0008, 0.0008, 0.0007, 0.0008, 0.0009, 0.0009, 0.0011,\n",
            "         0.0008, 0.0008, 0.0008, 0.0010, 0.0008, 0.0008, 0.0008, 0.0008, 0.0010,\n",
            "         0.0012, 0.0010, 0.0009, 0.0008, 0.0008, 0.0009, 0.0010, 0.0013, 0.0009,\n",
            "         0.0009, 0.0010, 0.0009, 0.0009, 0.0010, 0.0010, 0.0010, 0.0009, 0.0009,\n",
            "         0.0010, 0.0011, 0.0011, 0.0008, 0.0009, 0.0008, 0.0010, 0.0009, 0.0009,\n",
            "         0.0010, 0.0010, 0.0010, 0.0009, 0.0009, 0.0011, 0.0008, 0.0009, 0.0010,\n",
            "         0.0010, 0.0008, 0.0011, 0.0010, 0.0011, 0.0009, 0.0010, 0.0009, 0.0010,\n",
            "         0.0012, 0.0010, 0.0010, 0.0010, 0.0008, 0.0010, 0.0011, 0.0010, 0.0010,\n",
            "         0.0010, 0.0010, 0.0009, 0.0008, 0.0010, 0.0009, 0.0010, 0.0007, 0.0009,\n",
            "         0.0010, 0.0009, 0.0009, 0.0008, 0.0010, 0.0010, 0.0009, 0.0009, 0.0009,\n",
            "         0.0008, 0.0009, 0.0009, 0.0011, 0.0010, 0.0009, 0.0009, 0.0011, 0.0008,\n",
            "         0.0010, 0.0011, 0.0012, 0.0009, 0.0011, 0.0010, 0.0011, 0.0010, 0.0012,\n",
            "         0.0009, 0.0009, 0.0010, 0.0011, 0.0010, 0.0011, 0.0012, 0.0009, 0.0011,\n",
            "         0.0009, 0.0011, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0013,\n",
            "         0.0009, 0.0013, 0.0010, 0.0008, 0.0010, 0.0009, 0.0010, 0.0011, 0.0010,\n",
            "         0.0009, 0.0010, 0.0009, 0.0010, 0.0013, 0.0010, 0.0012, 0.0010, 0.0008,\n",
            "         0.0008, 0.0008, 0.0010, 0.0012, 0.0009, 0.0009, 0.0010, 0.0010, 0.0007,\n",
            "         0.0008, 0.0010, 0.0009, 0.0011, 0.0009, 0.0009, 0.0011, 0.0009, 0.0009,\n",
            "         0.0009, 0.0009, 0.0011, 0.0011, 0.0010, 0.0009, 0.0010, 0.0010, 0.0009,\n",
            "         0.0009, 0.0009, 0.0010, 0.0008, 0.0009, 0.0009, 0.0010, 0.0010, 0.0009,\n",
            "         0.0011, 0.0010, 0.0011, 0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
            "         0.0009, 0.0009, 0.0011, 0.0009, 0.0010, 0.0009, 0.0009, 0.0010, 0.0008,\n",
            "         0.0009, 0.0008, 0.0009, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "         0.0009, 0.0010, 0.0009, 0.0011, 0.0009, 0.0011, 0.0008, 0.0009, 0.0012,\n",
            "         0.0010, 0.0009, 0.0009, 0.0013, 0.0009, 0.0010, 0.0011, 0.0010, 0.0010,\n",
            "         0.0008, 0.0009, 0.0010, 0.0009, 0.0012, 0.0011, 0.0008, 0.0010, 0.0008,\n",
            "         0.0008, 0.0011, 0.0010, 0.0012, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "         0.0008, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0008, 0.0008,\n",
            "         0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0009, 0.0008, 0.0010, 0.0009,\n",
            "         0.0009, 0.0010, 0.0010, 0.0009, 0.0011, 0.0008, 0.0008, 0.0009, 0.0011,\n",
            "         0.0009, 0.0011, 0.0011, 0.0008, 0.0009, 0.0012, 0.0009, 0.0009, 0.0009,\n",
            "         0.0009, 0.0008, 0.0012, 0.0011, 0.0013, 0.0008, 0.0010, 0.0012, 0.0010,\n",
            "         0.0009, 0.0012, 0.0008, 0.0012, 0.0012, 0.0014, 0.0014, 0.0012, 0.0011,\n",
            "         0.0012, 0.0011, 0.0008, 0.0011, 0.0010, 0.0014, 0.0011, 0.0008, 0.0009,\n",
            "         0.0013, 0.0009, 0.0012, 0.0011, 0.0009, 0.0010, 0.0010, 0.0011, 0.0013,\n",
            "         0.0011, 0.0010, 0.0012, 0.0010, 0.0011, 0.0011, 0.0012, 0.0011, 0.0010,\n",
            "         0.0009, 0.0011, 0.0009, 0.0010, 0.0012, 0.0013, 0.0010, 0.0013, 0.0007,\n",
            "         0.0010, 0.0009, 0.0011, 0.0011, 0.0015, 0.0009, 0.0009, 0.0009, 0.0010,\n",
            "         0.0010, 0.0011, 0.0013, 0.0011, 0.0010, 0.0011, 0.0010, 0.0010, 0.0011,\n",
            "         0.0011, 0.0011, 0.0010, 0.0010, 0.0007, 0.0010, 0.0010, 0.0009, 0.0010,\n",
            "         0.0010, 0.0011, 0.0011, 0.0009, 0.0012, 0.0011, 0.0011, 0.0008, 0.0010,\n",
            "         0.0008, 0.0011, 0.0011, 0.0011, 0.0012, 0.0008, 0.0010, 0.0010, 0.0010,\n",
            "         0.0013, 0.0011, 0.0009, 0.0011, 0.0010, 0.0009, 0.0009, 0.0009, 0.0011,\n",
            "         0.0010, 0.0011, 0.0014, 0.0009, 0.0010, 0.0008, 0.0008, 0.0008, 0.0011,\n",
            "         0.0009, 0.0014, 0.0011, 0.0009, 0.0010, 0.0012, 0.0013, 0.0011, 0.0013,\n",
            "         0.0011, 0.0009, 0.0009, 0.0008, 0.0007, 0.0009, 0.0009, 0.0009, 0.0010,\n",
            "         0.0009, 0.0015, 0.0012, 0.0009, 0.0010, 0.0015, 0.0010, 0.0008, 0.0008,\n",
            "         0.0009, 0.0010, 0.0010, 0.0013, 0.0009, 0.0011, 0.0010, 0.0009, 0.0012,\n",
            "         0.0010, 0.0013, 0.0010, 0.0009, 0.0012, 0.0010, 0.0009, 0.0010, 0.0010,\n",
            "         0.0011, 0.0011, 0.0008, 0.0013, 0.0010, 0.0011, 0.0010, 0.0010, 0.0011,\n",
            "         0.0009, 0.0013, 0.0008, 0.0010, 0.0009, 0.0009, 0.0010, 0.0011, 0.0009,\n",
            "         0.0012, 0.0008, 0.0012, 0.0010, 0.0011, 0.0008, 0.0010, 0.0010, 0.0010,\n",
            "         0.0013, 0.0009, 0.0012, 0.0008, 0.0008, 0.0009, 0.0014, 0.0010, 0.0008,\n",
            "         0.0011, 0.0015, 0.0011, 0.0012, 0.0011, 0.0009, 0.0010, 0.0010, 0.0012,\n",
            "         0.0010, 0.0013, 0.0010, 0.0008, 0.0010, 0.0012, 0.0013, 0.0011, 0.0012,\n",
            "         0.0010, 0.0011, 0.0009, 0.0011, 0.0010, 0.0017, 0.0010, 0.0009, 0.0012,\n",
            "         0.0008, 0.0010, 0.0011, 0.0011, 0.0008, 0.0012, 0.0009, 0.0011, 0.0010,\n",
            "         0.0010, 0.0008, 0.0014, 0.0011, 0.0012, 0.0011, 0.0011, 0.0011, 0.0010,\n",
            "         0.0010, 0.0010, 0.0013, 0.0009, 0.0011, 0.0011, 0.0012, 0.0010, 0.0011,\n",
            "         0.0011, 0.0011, 0.0010, 0.0010, 0.0011, 0.0009, 0.0009, 0.0010, 0.0010,\n",
            "         0.0013, 0.0010, 0.0009, 0.0009, 0.0009, 0.0008, 0.0011, 0.0011, 0.0010,\n",
            "         0.0009, 0.0011, 0.0009, 0.0012, 0.0011, 0.0012, 0.0011, 0.0010, 0.0012,\n",
            "         0.0010, 0.0008, 0.0009, 0.0009, 0.0012, 0.0008, 0.0010, 0.0010, 0.0011,\n",
            "         0.0010, 0.0011, 0.0010, 0.0013, 0.0010, 0.0009, 0.0012, 0.0013, 0.0010,\n",
            "         0.0009, 0.0009, 0.0010, 0.0012, 0.0009, 0.0010, 0.0011, 0.0012, 0.0011,\n",
            "         0.0011, 0.0010, 0.0010, 0.0010, 0.0008, 0.0011, 0.0009, 0.0010, 0.0011,\n",
            "         0.0013, 0.0010, 0.0010, 0.0010, 0.0009, 0.0011, 0.0008, 0.0009, 0.0011,\n",
            "         0.0009, 0.0008, 0.0016, 0.0011, 0.0012, 0.0010, 0.0010, 0.0011, 0.0011,\n",
            "         0.0011, 0.0010, 0.0010, 0.0008, 0.0010, 0.0007, 0.0013, 0.0010, 0.0009,\n",
            "         0.0013, 0.0011, 0.0015, 0.0008, 0.0010, 0.0011, 0.0010, 0.0011, 0.0010,\n",
            "         0.0008, 0.0011, 0.0011, 0.0009, 0.0010, 0.0014, 0.0011, 0.0010, 0.0013,\n",
            "         0.0011, 0.0010, 0.0009, 0.0008, 0.0015, 0.0009, 0.0009, 0.0013, 0.0012,\n",
            "         0.0009, 0.0011, 0.0010, 0.0009, 0.0012, 0.0010, 0.0009, 0.0010, 0.0010,\n",
            "         0.0010, 0.0013, 0.0012, 0.0013, 0.0011, 0.0009, 0.0010, 0.0010, 0.0010,\n",
            "         0.0013, 0.0011, 0.0007, 0.0010, 0.0012, 0.0011, 0.0011, 0.0010, 0.0011,\n",
            "         0.0011, 0.0007, 0.0008, 0.0010, 0.0011, 0.0010, 0.0012, 0.0013, 0.0011,\n",
            "         0.0008, 0.0009, 0.0009, 0.0014, 0.0009, 0.0009, 0.0009, 0.0008, 0.0012,\n",
            "         0.0010, 0.0009, 0.0010, 0.0013, 0.0014, 0.0010, 0.0008, 0.0009, 0.0012,\n",
            "         0.0014, 0.0009, 0.0011, 0.0011, 0.0012, 0.0010, 0.0010, 0.0011, 0.0012,\n",
            "         0.0014, 0.0011, 0.0010, 0.0013, 0.0010, 0.0010, 0.0011, 0.0010, 0.0012,\n",
            "         0.0011, 0.0009, 0.0009, 0.0011, 0.0013, 0.0014, 0.0014, 0.0008, 0.0010,\n",
            "         0.0012, 0.0009, 0.0009, 0.0008, 0.0013, 0.0010, 0.0012, 0.0010, 0.0013,\n",
            "         0.0010, 0.0010, 0.0011, 0.0008, 0.0011, 0.0010, 0.0011, 0.0010, 0.0012,\n",
            "         0.0009, 0.0008, 0.0012, 0.0011, 0.0010, 0.0009, 0.0014, 0.0012, 0.0009,\n",
            "         0.0011, 0.0010, 0.0009, 0.0008, 0.0008, 0.0009, 0.0009, 0.0011, 0.0008,\n",
            "         0.0009, 0.0010, 0.0008, 0.0010, 0.0010, 0.0011, 0.0010, 0.0014, 0.0012,\n",
            "         0.0010, 0.0009, 0.0014, 0.0012, 0.0010, 0.0008, 0.0009, 0.0012, 0.0009,\n",
            "         0.0008, 0.0014, 0.0010, 0.0008, 0.0010, 0.0007, 0.0011, 0.0008, 0.0009,\n",
            "         0.0012, 0.0010, 0.0011, 0.0009, 0.0008, 0.0011, 0.0009, 0.0010, 0.0008,\n",
            "         0.0009, 0.0011, 0.0013, 0.0009, 0.0010, 0.0012, 0.0009, 0.0013, 0.0010,\n",
            "         0.0009, 0.0011, 0.0008, 0.0009, 0.0009, 0.0009, 0.0009, 0.0012, 0.0008,\n",
            "         0.0009, 0.0010, 0.0010, 0.0011, 0.0011, 0.0009, 0.0008, 0.0009, 0.0011,\n",
            "         0.0012, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0009, 0.0008,\n",
            "         0.0010, 0.0010, 0.0011, 0.0011, 0.0010, 0.0009, 0.0010, 0.0009, 0.0013,\n",
            "         0.0010, 0.0009, 0.0009, 0.0009, 0.0010, 0.0010, 0.0008, 0.0008, 0.0009,\n",
            "         0.0008, 0.0008, 0.0008, 0.0009, 0.0008, 0.0008, 0.0014, 0.0011, 0.0009,\n",
            "         0.0010, 0.0009, 0.0009, 0.0008, 0.0009, 0.0010, 0.0010, 0.0009, 0.0011,\n",
            "         0.0010]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the ResNet50 model predicted class along with its probability\n",
        "\n",
        "# YOUR CODE HERE\n",
        "ResNet50Model = create_model('resnet50d', pretrained=True)\n",
        "output = ResNet50Model(img)\n",
        "softmax = output.softmax(1)\n",
        "print(softmax)"
      ],
      "metadata": {
        "id": "BP_or0WhEhyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59e1c92-95ad-4732-b82f-0bfc9b562265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0011, 0.0009, 0.0011, 0.0012, 0.0013, 0.0013, 0.0010, 0.0009, 0.0008,\n",
            "         0.0009, 0.0011, 0.0011, 0.0010, 0.0011, 0.0010, 0.0010, 0.0012, 0.0010,\n",
            "         0.0011, 0.0009, 0.0010, 0.0010, 0.0010, 0.0009, 0.0009, 0.0009, 0.0011,\n",
            "         0.0011, 0.0009, 0.0013, 0.0008, 0.0012, 0.0009, 0.0009, 0.0010, 0.0008,\n",
            "         0.0010, 0.0010, 0.0011, 0.0012, 0.0011, 0.0010, 0.0009, 0.0010, 0.0011,\n",
            "         0.0010, 0.0010, 0.0011, 0.0009, 0.0009, 0.0009, 0.0009, 0.0014, 0.0011,\n",
            "         0.0009, 0.0011, 0.0009, 0.0010, 0.0011, 0.0013, 0.0011, 0.0012, 0.0010,\n",
            "         0.0014, 0.0012, 0.0013, 0.0012, 0.0011, 0.0010, 0.0012, 0.0009, 0.0010,\n",
            "         0.0010, 0.0010, 0.0008, 0.0011, 0.0009, 0.0008, 0.0011, 0.0009, 0.0009,\n",
            "         0.0008, 0.0009, 0.0008, 0.0010, 0.0008, 0.0010, 0.0012, 0.0008, 0.0010,\n",
            "         0.0008, 0.0009, 0.0010, 0.0010, 0.0009, 0.0011, 0.0010, 0.0008, 0.0009,\n",
            "         0.0010, 0.0009, 0.0011, 0.0008, 0.0016, 0.0011, 0.0010, 0.0010, 0.0010,\n",
            "         0.0008, 0.0010, 0.0009, 0.0013, 0.0013, 0.0012, 0.0009, 0.0008, 0.0008,\n",
            "         0.0009, 0.0010, 0.0009, 0.0010, 0.0010, 0.0008, 0.0008, 0.0008, 0.0009,\n",
            "         0.0011, 0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0011, 0.0008,\n",
            "         0.0007, 0.0008, 0.0009, 0.0009, 0.0009, 0.0008, 0.0010, 0.0010, 0.0008,\n",
            "         0.0008, 0.0009, 0.0012, 0.0011, 0.0009, 0.0012, 0.0009, 0.0013, 0.0011,\n",
            "         0.0011, 0.0011, 0.0011, 0.0010, 0.0014, 0.0013, 0.0012, 0.0010, 0.0009,\n",
            "         0.0010, 0.0011, 0.0011, 0.0010, 0.0011, 0.0008, 0.0012, 0.0012, 0.0009,\n",
            "         0.0017, 0.0016, 0.0011, 0.0012, 0.0008, 0.0012, 0.0010, 0.0014, 0.0011,\n",
            "         0.0011, 0.0011, 0.0009, 0.0010, 0.0009, 0.0011, 0.0010, 0.0011, 0.0010,\n",
            "         0.0009, 0.0011, 0.0008, 0.0010, 0.0010, 0.0011, 0.0012, 0.0012, 0.0009,\n",
            "         0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0012, 0.0010, 0.0009, 0.0009,\n",
            "         0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0011, 0.0009, 0.0011, 0.0011,\n",
            "         0.0012, 0.0009, 0.0010, 0.0009, 0.0013, 0.0008, 0.0014, 0.0011, 0.0011,\n",
            "         0.0011, 0.0010, 0.0011, 0.0011, 0.0012, 0.0012, 0.0011, 0.0012, 0.0010,\n",
            "         0.0012, 0.0011, 0.0014, 0.0013, 0.0010, 0.0009, 0.0011, 0.0011, 0.0012,\n",
            "         0.0010, 0.0008, 0.0010, 0.0011, 0.0011, 0.0012, 0.0011, 0.0011, 0.0010,\n",
            "         0.0009, 0.0012, 0.0011, 0.0008, 0.0010, 0.0011, 0.0011, 0.0011, 0.0008,\n",
            "         0.0011, 0.0009, 0.0010, 0.0010, 0.0009, 0.0008, 0.0009, 0.0009, 0.0011,\n",
            "         0.0013, 0.0010, 0.0012, 0.0011, 0.0009, 0.0009, 0.0009, 0.0011, 0.0012,\n",
            "         0.0014, 0.0011, 0.0011, 0.0009, 0.0008, 0.0011, 0.0011, 0.0010, 0.0009,\n",
            "         0.0010, 0.0008, 0.0009, 0.0008, 0.0009, 0.0010, 0.0012, 0.0011, 0.0011,\n",
            "         0.0011, 0.0011, 0.0013, 0.0008, 0.0011, 0.0008, 0.0009, 0.0008, 0.0008,\n",
            "         0.0011, 0.0010, 0.0008, 0.0009, 0.0010, 0.0008, 0.0009, 0.0011, 0.0010,\n",
            "         0.0009, 0.0009, 0.0009, 0.0011, 0.0008, 0.0010, 0.0011, 0.0009, 0.0009,\n",
            "         0.0011, 0.0009, 0.0009, 0.0008, 0.0010, 0.0009, 0.0011, 0.0010, 0.0011,\n",
            "         0.0009, 0.0008, 0.0011, 0.0009, 0.0012, 0.0008, 0.0010, 0.0010, 0.0009,\n",
            "         0.0008, 0.0009, 0.0012, 0.0010, 0.0011, 0.0009, 0.0011, 0.0008, 0.0010,\n",
            "         0.0010, 0.0010, 0.0012, 0.0011, 0.0008, 0.0016, 0.0011, 0.0013, 0.0012,\n",
            "         0.0011, 0.0010, 0.0010, 0.0011, 0.0011, 0.0012, 0.0008, 0.0014, 0.0009,\n",
            "         0.0010, 0.0011, 0.0012, 0.0009, 0.0014, 0.0011, 0.0011, 0.0009, 0.0010,\n",
            "         0.0009, 0.0009, 0.0012, 0.0010, 0.0013, 0.0015, 0.0012, 0.0011, 0.0012,\n",
            "         0.0010, 0.0009, 0.0012, 0.0013, 0.0011, 0.0010, 0.0009, 0.0012, 0.0014,\n",
            "         0.0009, 0.0010, 0.0009, 0.0011, 0.0012, 0.0010, 0.0009, 0.0013, 0.0013,\n",
            "         0.0013, 0.0008, 0.0009, 0.0008, 0.0010, 0.0009, 0.0010, 0.0009, 0.0011,\n",
            "         0.0013, 0.0007, 0.0010, 0.0009, 0.0011, 0.0009, 0.0011, 0.0010, 0.0009,\n",
            "         0.0009, 0.0010, 0.0008, 0.0008, 0.0011, 0.0011, 0.0010, 0.0008, 0.0012,\n",
            "         0.0014, 0.0009, 0.0015, 0.0010, 0.0006, 0.0010, 0.0012, 0.0009, 0.0009,\n",
            "         0.0010, 0.0009, 0.0011, 0.0009, 0.0009, 0.0009, 0.0014, 0.0010, 0.0010,\n",
            "         0.0008, 0.0015, 0.0010, 0.0008, 0.0011, 0.0008, 0.0013, 0.0014, 0.0010,\n",
            "         0.0011, 0.0013, 0.0012, 0.0012, 0.0013, 0.0013, 0.0014, 0.0010, 0.0006,\n",
            "         0.0010, 0.0011, 0.0016, 0.0011, 0.0007, 0.0012, 0.0011, 0.0010, 0.0007,\n",
            "         0.0011, 0.0010, 0.0007, 0.0009, 0.0008, 0.0009, 0.0011, 0.0007, 0.0009,\n",
            "         0.0009, 0.0009, 0.0019, 0.0009, 0.0010, 0.0009, 0.0008, 0.0007, 0.0008,\n",
            "         0.0007, 0.0007, 0.0008, 0.0010, 0.0010, 0.0009, 0.0012, 0.0010, 0.0010,\n",
            "         0.0009, 0.0011, 0.0013, 0.0009, 0.0011, 0.0008, 0.0011, 0.0007, 0.0010,\n",
            "         0.0012, 0.0010, 0.0014, 0.0008, 0.0011, 0.0009, 0.0007, 0.0011, 0.0007,\n",
            "         0.0011, 0.0010, 0.0011, 0.0011, 0.0007, 0.0007, 0.0009, 0.0013, 0.0010,\n",
            "         0.0007, 0.0008, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0008, 0.0008,\n",
            "         0.0011, 0.0009, 0.0010, 0.0010, 0.0007, 0.0010, 0.0010, 0.0010, 0.0009,\n",
            "         0.0008, 0.0011, 0.0007, 0.0009, 0.0010, 0.0011, 0.0008, 0.0010, 0.0011,\n",
            "         0.0012, 0.0010, 0.0008, 0.0010, 0.0012, 0.0010, 0.0009, 0.0007, 0.0008,\n",
            "         0.0008, 0.0010, 0.0007, 0.0012, 0.0009, 0.0013, 0.0008, 0.0012, 0.0008,\n",
            "         0.0009, 0.0009, 0.0011, 0.0010, 0.0009, 0.0010, 0.0007, 0.0009, 0.0014,\n",
            "         0.0008, 0.0009, 0.0012, 0.0009, 0.0009, 0.0006, 0.0009, 0.0011, 0.0010,\n",
            "         0.0010, 0.0009, 0.0010, 0.0011, 0.0009, 0.0010, 0.0025, 0.0009, 0.0015,\n",
            "         0.0011, 0.0013, 0.0008, 0.0013, 0.0010, 0.0010, 0.0008, 0.0013, 0.0006,\n",
            "         0.0009, 0.0011, 0.0008, 0.0010, 0.0013, 0.0011, 0.0010, 0.0011, 0.0008,\n",
            "         0.0008, 0.0010, 0.0010, 0.0009, 0.0009, 0.0011, 0.0007, 0.0008, 0.0011,\n",
            "         0.0009, 0.0008, 0.0008, 0.0010, 0.0007, 0.0008, 0.0012, 0.0009, 0.0012,\n",
            "         0.0009, 0.0008, 0.0010, 0.0008, 0.0009, 0.0010, 0.0007, 0.0012, 0.0011,\n",
            "         0.0011, 0.0011, 0.0011, 0.0009, 0.0009, 0.0011, 0.0009, 0.0010, 0.0008,\n",
            "         0.0009, 0.0007, 0.0009, 0.0010, 0.0007, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "         0.0012, 0.0010, 0.0008, 0.0011, 0.0007, 0.0009, 0.0010, 0.0009, 0.0010,\n",
            "         0.0007, 0.0013, 0.0012, 0.0011, 0.0010, 0.0010, 0.0009, 0.0010, 0.0014,\n",
            "         0.0010, 0.0010, 0.0007, 0.0009, 0.0010, 0.0007, 0.0009, 0.0013, 0.0010,\n",
            "         0.0008, 0.0009, 0.0011, 0.0009, 0.0011, 0.0010, 0.0010, 0.0017, 0.0012,\n",
            "         0.0012, 0.0008, 0.0008, 0.0009, 0.0007, 0.0010, 0.0010, 0.0009, 0.0010,\n",
            "         0.0010, 0.0008, 0.0009, 0.0008, 0.0012, 0.0012, 0.0007, 0.0011, 0.0012,\n",
            "         0.0008, 0.0006, 0.0010, 0.0008, 0.0009, 0.0013, 0.0012, 0.0008, 0.0014,\n",
            "         0.0007, 0.0008, 0.0010, 0.0009, 0.0012, 0.0008, 0.0011, 0.0006, 0.0007,\n",
            "         0.0007, 0.0010, 0.0012, 0.0007, 0.0009, 0.0012, 0.0011, 0.0010, 0.0007,\n",
            "         0.0010, 0.0011, 0.0014, 0.0009, 0.0009, 0.0008, 0.0013, 0.0008, 0.0012,\n",
            "         0.0011, 0.0009, 0.0010, 0.0009, 0.0011, 0.0009, 0.0009, 0.0014, 0.0011,\n",
            "         0.0008, 0.0009, 0.0012, 0.0008, 0.0010, 0.0009, 0.0010, 0.0010, 0.0009,\n",
            "         0.0008, 0.0010, 0.0013, 0.0011, 0.0015, 0.0008, 0.0008, 0.0008, 0.0008,\n",
            "         0.0011, 0.0012, 0.0013, 0.0010, 0.0009, 0.0008, 0.0009, 0.0012, 0.0008,\n",
            "         0.0013, 0.0014, 0.0013, 0.0008, 0.0012, 0.0009, 0.0010, 0.0011, 0.0006,\n",
            "         0.0012, 0.0007, 0.0007, 0.0012, 0.0012, 0.0007, 0.0011, 0.0012, 0.0007,\n",
            "         0.0009, 0.0009, 0.0009, 0.0010, 0.0009, 0.0011, 0.0009, 0.0007, 0.0011,\n",
            "         0.0010, 0.0010, 0.0009, 0.0011, 0.0010, 0.0008, 0.0010, 0.0011, 0.0010,\n",
            "         0.0015, 0.0009, 0.0010, 0.0008, 0.0009, 0.0010, 0.0011, 0.0011, 0.0012,\n",
            "         0.0012, 0.0010, 0.0011, 0.0015, 0.0012, 0.0012, 0.0011, 0.0013, 0.0011,\n",
            "         0.0011, 0.0012, 0.0009, 0.0010, 0.0010, 0.0007, 0.0014, 0.0010, 0.0011,\n",
            "         0.0008, 0.0010, 0.0008, 0.0009, 0.0007, 0.0009, 0.0008, 0.0009, 0.0010,\n",
            "         0.0006, 0.0007, 0.0007, 0.0008, 0.0011, 0.0008, 0.0010, 0.0007, 0.0012,\n",
            "         0.0009, 0.0007, 0.0010, 0.0011, 0.0008, 0.0009, 0.0013, 0.0011, 0.0008,\n",
            "         0.0009, 0.0013, 0.0009, 0.0008, 0.0007, 0.0010, 0.0009, 0.0011, 0.0008,\n",
            "         0.0007, 0.0009, 0.0008, 0.0011, 0.0009, 0.0013, 0.0010, 0.0014, 0.0017,\n",
            "         0.0011, 0.0014, 0.0013, 0.0010, 0.0009, 0.0009, 0.0011, 0.0010, 0.0010,\n",
            "         0.0007, 0.0009, 0.0008, 0.0009, 0.0009, 0.0007, 0.0007, 0.0007, 0.0006,\n",
            "         0.0010, 0.0013, 0.0012, 0.0006, 0.0011, 0.0008, 0.0009, 0.0011, 0.0007,\n",
            "         0.0008, 0.0008, 0.0009, 0.0010, 0.0010, 0.0010, 0.0008, 0.0011, 0.0008,\n",
            "         0.0011, 0.0010, 0.0011, 0.0011, 0.0008, 0.0008, 0.0010, 0.0009, 0.0010,\n",
            "         0.0010, 0.0008, 0.0012, 0.0009, 0.0011, 0.0011, 0.0013, 0.0011, 0.0008,\n",
            "         0.0012, 0.0008, 0.0012, 0.0012, 0.0010, 0.0011, 0.0009, 0.0011, 0.0007,\n",
            "         0.0008, 0.0008, 0.0008, 0.0013, 0.0013, 0.0011, 0.0010, 0.0009, 0.0010,\n",
            "         0.0009, 0.0010, 0.0012, 0.0008, 0.0010, 0.0011, 0.0012, 0.0009, 0.0012,\n",
            "         0.0008, 0.0007, 0.0010, 0.0011, 0.0010, 0.0010, 0.0013, 0.0011, 0.0010,\n",
            "         0.0009, 0.0009, 0.0009, 0.0007, 0.0007, 0.0011, 0.0011, 0.0008, 0.0009,\n",
            "         0.0014]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùì ***Question: What classes are predicted for the two models? Are the models confident? Is it a good thing?***"
      ],
      "metadata": {
        "id": "IBxaFdfdwnpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù YOUR ANSWERS GO HERE"
      ],
      "metadata": {
        "id": "hjw-R-FdxlFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Instantiating the dataloaders to perform fine tuning\n",
        "When working with large image datasets with PyTorch, people often implement a DataLoader to help manage how images are loaded during training. The dataloader can be combined with methods that implement data augmentation by modifying the images with transforms (eg. scaling, rotation, reflection, cropping, etc.). For the most common applications, there are existing data loaders that are perfectly suitable and that users can use instead of defining their own.\n"
      ],
      "metadata": {
        "id": "soeLJWebR8Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "NZROCPrvJ8sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Using the `ImageFolder` strategy to build a dataloader with a batch size of 128 for training. ([This tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#afterword-torchvision) is most helpful.)\n",
        "\n",
        "You will also want your dataloader for your training set to apply the following data augmentation transforms (documentation available [here](https://pytorch.org/vision/stable/transforms.html)):\n",
        "\n",
        "1. Random rotation between -10 and 10\n",
        "2. Random horizontal flip with 40% probability"
      ],
      "metadata": {
        "id": "U0yxnCVsKiIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates the dataset and dataloader that will be used for training\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomHorizontalFlip(p=0.4),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "training_dataset = ImageFolder(root='/content/SYSC4415W23_A2_dataset/training',\n",
        "                               transform=data_transform)\n",
        "training_loader = DataLoader(training_dataset, batch_size =batch_size, shuffle =True)"
      ],
      "metadata": {
        "id": "Acm52Rn_ayUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2**: Create the dataloaders you will be using for validation and testing. The transform should only convert the images to a tensor. You should not specify a batch size for the test set dataloader."
      ],
      "metadata": {
        "id": "fj5c1hYV8Oxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates the dataloader that will be used for validation\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "validation_dataset = ImageFolder(root='/content/SYSC4415W23_A2_dataset/validation',\n",
        "                               transform=data_transform)\n",
        "validation_loader = DataLoader(training_dataset, batch_size =batch_size, shuffle =True)"
      ],
      "metadata": {
        "id": "DmGqrtFN6p7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates the dataloader that will be used testing\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "test_dataset = ImageFolder(root='/content/SYSC4415W23_A2_dataset/test',\n",
        "                               transform=data_transform)\n",
        "test_loader = DataLoader(training_dataset,\n",
        "                         batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "XX-Gkx8iK7XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Fine-tuning existing CNN architectures\n",
        "\n",
        "The Inception V3 and ResNet50 models you loaded above were trained on ImageNet which is not a medical dataset. In order to leverage these models for our purposes, we need to modify the architecture so that the final classification layer contains an appropriate number of classes and retune the model weights so that the models become suitable for the classification of our x-rays.\n"
      ],
      "metadata": {
        "id": "Xt5LRQ1B8i39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timm import create_model\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch import no_grad, Tensor\n",
        "from torch import cuda"
      ],
      "metadata": {
        "id": "LN6ZeKE8Lp-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Using `create_model` from the `timm` package, Load the InceptionV3 and ResNet50 models, replacing the final layer with one appropriate for our purpose (recall that we want to classify x-rays of healthy, pneumonia and SARS-CoV-3 patients). Note that the timm library can assist in replacing the final layer (see [Anthony's tutorial](https://github.com/jrgreen7/SYSC4906/blob/master/W2023/Tutorials/CIFAR100_tutorial_WIP.ipynb)). These are your modified models."
      ],
      "metadata": {
        "id": "iTGT-CC5Los5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the InceptionV3 model and replaces the final classification layer with a new dense layer\n",
        "\n",
        "# YOUR CODE HERE\n",
        "inceptionV3Model = create_model('inception_v3', num_classes=3, pretrained=True)"
      ],
      "metadata": {
        "id": "FyUSzYSk89Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the ResNet50 model and replaces the final classification layer with a new dense layer\n",
        "\n",
        "# YOUR CODE HERE\n",
        "ResNet50Model = create_model('resnet50d', num_classes=3, pretrained=True)"
      ],
      "metadata": {
        "id": "X0nCkWigU0dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** In a tutorial with Anthony, you have seen that you can convert a base learning rate to an effective learning rate based on the batch size you have selected using the following heuristic:\n",
        "\n",
        "$$\\eta_{eff} = \\frac{B\\eta_{base}}{256}$$\n",
        "\n",
        "where $\\eta$ is the learning rate and $B$ is the batch size.\n",
        "\n",
        "Train the final layer modified InceptionV3 and ResNet50 models on your training set. Use the **base** learning rate $\\eta_{base}$ of 0.0005.\n",
        "\n",
        "Use the following settings:\n",
        "\n",
        "**Epochs:** 25\n",
        "\n",
        "**Optimizer:** AdamW\n",
        "\n",
        "**Loss function:** Cross-entropy (it is not required here, but note that using the weight parameter here could help deal with class imbalance)\n",
        "\n",
        "Implement the training loop yourself. Do not use a package that automates the process. Anthony has demonstrated how to do this and much can be taken from [his example](https://github.com/jrgreen7/SYSC4906/blob/master/W2023/Tutorials/CIFAR100_tutorial_WIP.ipynb).\n",
        "\n",
        "‚ùó**Important: Make sure you are using a colab gpu and to store the mean training and validation performance/loss at each epoch as you will be plotting them in the next steps.**"
      ],
      "metadata": {
        "id": "fnPggZay-S_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Moves the modified inceptionV3 model to GPU\n",
        "\n",
        "# YOUR CODE HERE\n",
        "inceptionV3Model.cuda()\n",
        "\n"
      ],
      "metadata": {
        "id": "Xpt117nYLyPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04da747d-7b1c-4abb-dfb8-60be1abd2e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InceptionV3(\n",
              "  (Conv2d_1a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_2a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_2b_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (Conv2d_3b_1x1): BasicConv2d(\n",
              "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_4a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (Mixed_5b): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_5c): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_5d): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6a): InceptionB(\n",
              "    (branch3x3): BasicConv2d(\n",
              "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6b): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6c): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6d): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6e): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7a): InceptionD(\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_4): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7b): InceptionE(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7c): InceptionE(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
              "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiates the optimizer for the modified InceptionV3 model using the specified effective learning rate\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "base_lr = 0.0005\n",
        "learning_rate = base_lr * batch_size / 256\n",
        "opt = AdamW(inceptionV3Model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "xPG5MGPIMk4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets up the loss function for the modified InceptionV3 model\n",
        "\n",
        "# YOUR CODE HERE\n",
        "loss_function = CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "EfcMH2x-Mpjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tunes the weights in the final layer the modified InceptionV3 model (main learning loop)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "epochs = 25\n",
        "inceptionV3TrainMean = []  # Store the mean training loss values from each epoch\n",
        "inceptionV3ValMean = []  # Store the mean validation loss values from each epoch\n",
        "\n",
        "#below was taken from anthony's tutorial\n",
        "for epoch in range(epochs):\n",
        "  start_time = time.time()\n",
        "\n",
        "  train_loss = []\n",
        "  inceptionV3Model.train()\n",
        "  for batch in training_loader:\n",
        "    batch_imgs, batch_labels = batch\n",
        "    batch_imgs = batch_imgs.cuda()\n",
        "    batch_labels = batch_labels.cuda()\n",
        "    logits = inceptionV3Model(batch_imgs)\n",
        "    loss = loss_function(logits, batch_labels)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "    train_loss.append(loss.item())\n",
        "\n",
        "  val_losses = []\n",
        "  inceptionV3Model.eval()  # put model in \"eval\" mode\n",
        "  with torch.no_grad():  # when evaluating, we don't need gradients\n",
        "      for batch in validation_loader:\n",
        "          batch_imgs, batch_labels = batch\n",
        "          batch_imgs = batch_imgs.cuda()  # (bsz, 3, 32, 32)\n",
        "          batch_labels = batch_labels.cuda()  # (bsz)\n",
        "\n",
        "          logits = inceptionV3Model(batch_imgs)  # (bsz, 100)\n",
        "          loss = loss_function(logits, batch_labels)\n",
        "          val_losses.append(loss.item())\n",
        "\n",
        "  train_time = time.time() - start_time\n",
        "  epoch_train_loss = torch.Tensor(train_loss).mean().item()\n",
        "  epoch_val_loss = torch.Tensor(val_losses).mean().item()\n",
        "\n",
        "  inceptionV3TrainMean.append(epoch_train_loss)\n",
        "  inceptionV3ValMean.append(epoch_val_loss)\n",
        "  print(f'Epoch: {epoch}  Train Loss: {epoch_train_loss:8.6f}   Val Loss: {epoch_val_loss:8.6f}  Time: {train_time:6.4f}')"
      ],
      "metadata": {
        "id": "D8tIT-9M-WkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d169259-18fb-4a23-b82c-9acb6914e16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0  Train Loss: 0.431590   Val Loss: 0.958650  Time: 13.9839\n",
            "Epoch: 1  Train Loss: 0.146051   Val Loss: 0.118562  Time: 10.3532\n",
            "Epoch: 2  Train Loss: 0.087482   Val Loss: 0.090354  Time: 10.2547\n",
            "Epoch: 3  Train Loss: 0.069915   Val Loss: 0.053129  Time: 10.4191\n",
            "Epoch: 4  Train Loss: 0.041009   Val Loss: 0.032283  Time: 10.4963\n",
            "Epoch: 5  Train Loss: 0.048203   Val Loss: 0.029192  Time: 10.4212\n",
            "Epoch: 6  Train Loss: 0.046471   Val Loss: 0.023052  Time: 10.3681\n",
            "Epoch: 7  Train Loss: 0.042797   Val Loss: 0.053499  Time: 10.3696\n",
            "Epoch: 8  Train Loss: 0.031839   Val Loss: 0.020437  Time: 10.3094\n",
            "Epoch: 9  Train Loss: 0.025683   Val Loss: 0.020162  Time: 10.3527\n",
            "Epoch: 10  Train Loss: 0.021905   Val Loss: 0.005838  Time: 10.3518\n",
            "Epoch: 11  Train Loss: 0.028921   Val Loss: 0.016792  Time: 10.2539\n",
            "Epoch: 12  Train Loss: 0.037239   Val Loss: 0.027712  Time: 10.3567\n",
            "Epoch: 13  Train Loss: 0.018520   Val Loss: 0.006079  Time: 10.3424\n",
            "Epoch: 14  Train Loss: 0.009737   Val Loss: 0.012546  Time: 10.3492\n",
            "Epoch: 15  Train Loss: 0.011875   Val Loss: 0.012498  Time: 10.2682\n",
            "Epoch: 16  Train Loss: 0.014295   Val Loss: 0.008683  Time: 10.2398\n",
            "Epoch: 17  Train Loss: 0.009066   Val Loss: 0.020889  Time: 10.3182\n",
            "Epoch: 18  Train Loss: 0.015970   Val Loss: 0.014083  Time: 10.3397\n",
            "Epoch: 19  Train Loss: 0.009128   Val Loss: 0.006004  Time: 10.3929\n",
            "Epoch: 20  Train Loss: 0.010788   Val Loss: 0.011093  Time: 10.2741\n",
            "Epoch: 21  Train Loss: 0.022019   Val Loss: 0.012916  Time: 10.2879\n",
            "Epoch: 22  Train Loss: 0.012232   Val Loss: 0.003525  Time: 10.4189\n",
            "Epoch: 23  Train Loss: 0.009023   Val Loss: 0.008998  Time: 10.2989\n",
            "Epoch: 24  Train Loss: 0.009248   Val Loss: 0.003329  Time: 10.2916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Moves the modified ResNet50 model to GPU\n",
        "\n",
        "# YOUR CODE HERE\n",
        "ResNet50Model.cuda()"
      ],
      "metadata": {
        "id": "O4OZyXmHxkFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0d1d73-8fe9-4f2a-c714-f4aa51c21466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  )\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act1): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Identity()\n",
              "        (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act3): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
              "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiates the optimizer for the modified ResNet50 model\n",
        "\n",
        "# YOUR CODE HERE\n",
        "base_lr = 0.0005\n",
        "learning_rate = base_lr * batch_size / 256\n",
        "opt = AdamW(ResNet50Model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Fz_V6au6RbV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets up the loss function for the modified ResNet50 model\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "loss_function = CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "W3iE55k-RbV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tunes the weights in the final layer the modified ResNet50 model (main learning loop)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "resNet50TrainMean = [] #store the mean loss values from each epoch\n",
        "resNet50ValMean = []\n",
        "\n",
        "#below was taken from anthony's tutorial\n",
        "for epoch in range(epochs):\n",
        "  start_time = time.time()\n",
        "  train_loss = []\n",
        "  ResNet50Model.train()\n",
        "  for batch in training_loader:\n",
        "    batch_imgs, batch_labels = batch\n",
        "    batch_imgs = batch_imgs.cuda()\n",
        "    batch_labels = batch_labels.cuda()\n",
        "    logits = ResNet50Model(batch_imgs)\n",
        "    loss = loss_function(logits, batch_labels)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "    train_loss.append(loss.item())\n",
        "\n",
        "  val_losses = []\n",
        "  ResNet50Model.eval()  # put model in \"eval\" mode\n",
        "  with torch.no_grad():\n",
        "      for batch in validation_loader:\n",
        "          batch_imgs, batch_labels = batch\n",
        "          batch_imgs = batch_imgs.cuda()\n",
        "          batch_labels = batch_labels.cuda()\n",
        "\n",
        "          logits = ResNet50Model(batch_imgs)\n",
        "          loss = loss_function(logits, batch_labels)\n",
        "          val_losses.append(loss.item())\n",
        "\n",
        "  train_time = time.time() - start_time\n",
        "  #find the mean training and validation losses for the current epoch\n",
        "  epoch_train_loss = torch.Tensor(train_loss).mean().item()\n",
        "  epoch_val_loss = torch.Tensor(val_losses).mean().item()\n",
        "\n",
        "  #append\n",
        "  resNet50TrainMean.append(epoch_train_loss)\n",
        "  resNet50ValMean.append(epoch_val_loss)\n",
        "  print(f'Epoch: {epoch}  Train Loss: {epoch_train_loss:8.6f}   Val Loss: {epoch_val_loss:8.6f}  Time: {train_time:6.4f}')"
      ],
      "metadata": {
        "id": "HOecqyEUFpnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa4f4b2-f268-45e7-d355-dbcda5a8f108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0  Train Loss: 0.960653   Val Loss: 0.946065  Time: 11.5292\n",
            "Epoch: 1  Train Loss: 0.638513   Val Loss: 0.804221  Time: 11.4284\n",
            "Epoch: 2  Train Loss: 0.391133   Val Loss: 0.430910  Time: 11.4264\n",
            "Epoch: 3  Train Loss: 0.248592   Val Loss: 0.206514  Time: 11.4148\n",
            "Epoch: 4  Train Loss: 0.166490   Val Loss: 0.138090  Time: 11.4080\n",
            "Epoch: 5  Train Loss: 0.126530   Val Loss: 0.105061  Time: 11.3938\n",
            "Epoch: 6  Train Loss: 0.107925   Val Loss: 0.082318  Time: 11.3575\n",
            "Epoch: 7  Train Loss: 0.085985   Val Loss: 0.067827  Time: 11.4241\n",
            "Epoch: 8  Train Loss: 0.069741   Val Loss: 0.048706  Time: 11.5141\n",
            "Epoch: 9  Train Loss: 0.056428   Val Loss: 0.047709  Time: 11.6426\n",
            "Epoch: 10  Train Loss: 0.051767   Val Loss: 0.035926  Time: 11.4889\n",
            "Epoch: 11  Train Loss: 0.040641   Val Loss: 0.029323  Time: 11.4432\n",
            "Epoch: 12  Train Loss: 0.041472   Val Loss: 0.023127  Time: 11.4148\n",
            "Epoch: 13  Train Loss: 0.032657   Val Loss: 0.019510  Time: 11.4928\n",
            "Epoch: 14  Train Loss: 0.033480   Val Loss: 0.021300  Time: 11.5481\n",
            "Epoch: 15  Train Loss: 0.057916   Val Loss: 0.025397  Time: 11.4582\n",
            "Epoch: 16  Train Loss: 0.031650   Val Loss: 0.024916  Time: 11.4499\n",
            "Epoch: 17  Train Loss: 0.024142   Val Loss: 0.016914  Time: 11.5808\n",
            "Epoch: 18  Train Loss: 0.015098   Val Loss: 0.013810  Time: 11.5875\n",
            "Epoch: 19  Train Loss: 0.015486   Val Loss: 0.011038  Time: 11.4600\n",
            "Epoch: 20  Train Loss: 0.015691   Val Loss: 0.014404  Time: 11.5120\n",
            "Epoch: 21  Train Loss: 0.015340   Val Loss: 0.010429  Time: 11.4703\n",
            "Epoch: 22  Train Loss: 0.009538   Val Loss: 0.007778  Time: 11.5450\n",
            "Epoch: 23  Train Loss: 0.016998   Val Loss: 0.007879  Time: 11.4089\n",
            "Epoch: 24  Train Loss: 0.024924   Val Loss: 0.007330  Time: 11.3838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** In different labeled subplots, display the learning curves for each model. Each subplot should display loss on the training set and the validation set (*i.e.* 2 curves per subplot). Use matplotlib."
      ],
      "metadata": {
        "id": "hWdTW9APDrpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displays the learning curves (loss) for both models in two separate subplots\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(inceptionV3TrainMean, label='Training Loss')\n",
        "ax.plot(inceptionV3ValMean, label='Validation Loss')\n",
        "\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Learning Curves')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l-uQC4sIX6-b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4738fa33-81b3-4b7d-825b-c4f6f74acd75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùì ***Question: Comment on your learning curves. What do they tell you?***\n",
        "\n",
        "The training loss and validation loss curves are very close to eachother throughout the training process. This shows us that the model has been trained well and that it is able to able to apply what it has learned to untrained data effectively"
      ],
      "metadata": {
        "id": "EvoZe_IGQJ__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù YOUR ANSWER GOES HERE"
      ],
      "metadata": {
        "id": "UTgGmtFzQVu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Performance evaluation on a test set\n",
        "\n",
        "Of course, estimating the performance of your model on unseen data is a key step in machine learning methodology. Here, you will summarize model performance for your InceptionV3 model and ResNet50 model on the test set."
      ],
      "metadata": {
        "id": "GBAcg-ezIKZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from torch import no_grad"
      ],
      "metadata": {
        "id": "qfL6yr-E32SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Iterate through the images in the test set using the test dataloader to make predictions for the test set images and retrieve their actual label (its index). Note that this is done in a way similar to the validation step in the training loop.\n",
        "\n",
        "Append the predicted class index to a list, the actual labels to another and the probability of the SARS-CoV-3 class to another list.\n",
        "\n",
        "Do this for both fine-tuned models.\n",
        "\n",
        "Note that the indices map to the classes as follows:\n",
        "\n",
        "0: normal\n",
        "\n",
        "1: pneumonia\n",
        "\n",
        "2: sarscov3"
      ],
      "metadata": {
        "id": "yfskXdis0a8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a list of predictions, a list of actual labels and a list of probabilities of the SARS-CoV-3 class for the fine-tuned InceptionV3 model applied to the test set\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# For InceptionV3 model\n",
        "\n",
        "# For InceptionV3 model\n",
        "inceptionV3Prediction = []\n",
        "inceptionV3Actual = []\n",
        "inceptionV3Probability = []\n",
        "\n",
        "inceptionV3Model.eval()\n",
        "with no_grad():\n",
        "  #iterate over test set\n",
        "    for batch in test_loader:\n",
        "        batch_imgs, batch_labels = batch\n",
        "\n",
        "        #load images and labels into gpu\n",
        "        batch_imgs = batch_imgs.cuda()\n",
        "        batch_labels = batch_labels.cuda()\n",
        "\n",
        "        #compute logits\n",
        "        logits = inceptionV3Model(batch_imgs)\n",
        "        #apply softmax to ensure probability distribution between [0, 1.0] adds ups to 1.0\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        #find class with highest probability\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        #append to list\n",
        "        inceptionV3Prediction.extend(preds.cpu().numpy())\n",
        "        inceptionV3Actual.extend(batch_labels.cpu().numpy())\n",
        "        inceptionV3Probability.extend(probs.cpu().numpy())\n",
        "\n",
        "#output results\n",
        "print(\"InceptionV3 Model Results:\")\n",
        "for index in range(len(inceptionV3Prediction)):\n",
        "    print(f'Index: {index} | Prediction: {inceptionV3Prediction[index]} | Actual: {inceptionV3Actual[index]} | Probability SARS-CoV-3: {inceptionV3Probability[index][2]:.2f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "euMGErLf0i_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a list of predictions, a list of actual labels and a list of probabilities of the SARS-CoV-3 class for the fine-tuned ResNet50 model applied to the test set\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# For ResNet50 model\n",
        "resnet50Prediction  = []\n",
        "resnet50Actual = []\n",
        "resnet50Probability = []\n",
        "\n",
        "ResNet50Model.eval()\n",
        "with no_grad():\n",
        "    #iterate over test set\n",
        "    for batch in test_loader:\n",
        "        batch_imgs, batch_labels = batch\n",
        "\n",
        "        #load images and labels into gpu\n",
        "        batch_imgs = batch_imgs.cuda()\n",
        "        batch_labels = batch_labels.cuda()\n",
        "\n",
        "        #compute logits\n",
        "        logits = ResNet50Model(batch_imgs)\n",
        "        #apply softmax to ensure probability distribution between [0, 1.0] adds ups to 1.0\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        #find class with highest probability\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        #append to list\n",
        "        resnet50Prediction.extend(preds.cpu().numpy())\n",
        "        resnet50Actual.extend(batch_labels.cpu().numpy())\n",
        "        resnet50Probability.extend(probs[:, 2].cpu().numpy())\n",
        "\n",
        "#output results\n",
        "print(\"\\nResNet50 Model Results:\")\n",
        "for index in range(len(resnet50Prediction)):\n",
        "    print(f'Index: {index} | Prediction: {resnet50Prediction[index]} | Actual: {resnet50Actual[index]} | Probability SARS-CoV-3: {resnet50Probability[index]:.2%}')\n",
        "\n"
      ],
      "metadata": {
        "id": "e6UMYl4W3FhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Show the confusion matrices for both models. The `confusion_matrix` function from scikit-learn I imported for you is useful for this."
      ],
      "metadata": {
        "id": "xl53hgxEE6PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates the confusion matrix for the modified InceptionV3 model\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "#NOTE: I know we are supposed to convert it to a 2x2 in the next step but I did it here just to make it easier to visualize for myself\n",
        "# and to make calculations easier for future steps\n",
        "inceptionV3CM = confusion_matrix(inceptionV3Actual, inceptionV3Prediction)\n",
        "print(\"\\nInceptionV3 Confusion Matrix:\")\n",
        "print(inceptionV3CM)\n",
        "\n",
        "tn1 = inceptionV3CM[0][0]\n",
        "fp1 = inceptionV3CM[0][1] + inceptionV3CM[0][2]\n",
        "fn1 = inceptionV3CM[1][0] + inceptionV3CM[2][0]\n",
        "tp1 = inceptionV3CM[1][1] + inceptionV3CM[1][2] + inceptionV3CM[2][1] + inceptionV3CM[2][2]\n",
        "\n",
        "print(\"\\n2x2 InceptionV3 Confusion Matrix:\")\n",
        "print(f\"TN: {tn1} | FP: {fp1}\")\n",
        "print(f\"FN: {fn1} | TP: {tp1}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ya11twapResF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c06be5b-f0c1-42ec-9914-a82e00f2eb4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "InceptionV3 Confusion Matrix:\n",
            "[[1033    0    0]\n",
            " [   2  843    0]\n",
            " [   0    0  344]]\n",
            "\n",
            "2x2 InceptionV3 Confusion Matrix:\n",
            "TN: 1033 | FP: 0\n",
            "FN: 2 | TP: 1187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates the confusion matrix for the modified ResNet50 model\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "ResNet50CM = confusion_matrix(resnet50Actual, resnet50Prediction)\n",
        "print(\"\\nResNet50 Confusion Matrix:\")\n",
        "print(ResNet50CM)\n",
        "\n",
        "\n",
        "tn2 = ResNet50CM[0][0]\n",
        "fp2 = ResNet50CM[0][1] + ResNet50CM[0][2]\n",
        "fn2 = ResNet50CM[1][0] + ResNet50CM[2][0]\n",
        "tp2 = ResNet50CM[1][1] + ResNet50CM[1][2] + ResNet50CM[2][1] + ResNet50CM[2][2]\n",
        "\n",
        "print(\"\\n2x2 ResNet50 Confusion Matrix:\")\n",
        "print(f\"TN: {tn2} | FP: {fp2}\")\n",
        "print(f\"FN: {fn2} | TP: {tp2}\")"
      ],
      "metadata": {
        "id": "ns6s6QGz4YIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d57dc96-6677-4fbb-e053-6a83d3490765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ResNet50 Confusion Matrix:\n",
            "[[1031    2    0]\n",
            " [   0  845    0]\n",
            " [   0    0  344]]\n",
            "\n",
            "2x2 ResNet50 Confusion Matrix:\n",
            "TN: 1031 | FP: 2\n",
            "FN: 0 | TP: 1189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùì ***Question: Based off these matrices, report the accuracy of the models.***\n",
        "\n",
        "based on these matrices we can say our models are very accurate. We are getting a high amount of true positives and true negatives and only a few false positives."
      ],
      "metadata": {
        "id": "5g_NVIx5RpOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù YOUR ANSWER GOES HERE"
      ],
      "metadata": {
        "id": "trfucb7V53fR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Use the `PrecisionRecallDisplay.from_predictions` methods documented [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#:~:text=The%20precision%2Drecall%20curve%20shows,a%20low%20false%20negative%20rate.) to plot the precision recall curves for your models. There are three classes, so convert your labels so that it becomes a binary classification scenario, ie. SARS-CoV-3 vs. not-SARS-CoV-3."
      ],
      "metadata": {
        "id": "7umfab-rFaOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots the PR curves of your fine-tuned InceptionV3 and ResNet50 models\n",
        "\n",
        "# YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "xKoSmJAJRe6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùì ***Question: Based off the results you obtained in this section, which model performs best? Why do you think (in 5 sentences or less)?***"
      ],
      "metadata": {
        "id": "CUHNYRuIylfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù YOUR ANSWER GOES HERE"
      ],
      "metadata": {
        "id": "s8m6O95LylqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Answering questions from investors ‚ùì\n",
        "\n",
        "Bfizer has heard about your model and are interested in investing in your technology. However, before they engage in further discussions, they want you to answer the following questions:\n",
        "\n",
        "1. Briefly provide techniques that you would explore next to further improve the performance of your model?\n",
        "\n",
        "2. A competitor has trained an SVM on the same dataset but performs worse than your model. Why do CNNs perform better than SVM for image classification? Discuss two aspects: differences in features and differences in training data.\n",
        "\n",
        "3. The investors consider investing in a very small device that can run your model. Would your best model fit on this small cost-effective device with 32 MB of storage, assuming that all parameters in the model are 16 bit floats (you can ignore everything in the model but the trainable parameters)? You can use the `torchstat` library ([link](https://github.com/Swall0w/torchstat)) to get the number of parameters. Note that the relevant function from this library was already imported for you (below).\n",
        "\n",
        "4. If your best model predict a positive SARS-CoV-3 case, how likely is it that you are correct? How likely is it that you are wrong?\n",
        "\n",
        "5. Assuming that the test data is representative of the disease status among the general population of Canada (it is not, why? hint: think of bias.), how many Canadians (Canada pop: 38M) currently have SARS-CoV-4? How much will it cost Canadians to inject all infected people if one dose of Greenraza‚Ñ¢Ô∏è can be purchased for 13 CAD (Canadian Dollars)? How many people have pneumonia and need to be isolated? (Show your calculations.)\n",
        "\n",
        "**Note: Your confusion matrices might be useful for the last 2 questions. üòâ**"
      ],
      "metadata": {
        "id": "1dfWbPP0HpnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù YOUR SHORT ANSWERS GO HERE (add code cells below as needed for calculations, eg. for running the `stat` function from the `torchstat` package)\n",
        "\n",
        "1) First I would try to get an improved dataset with more examples of sars-cov3. We could also look further into adjusting the hyperperameters such as learning rate or batch size in order to get better results. We can also try combining models using techniques like averaging, majority vote or stacking.\n",
        "\n",
        "2) CNNS are capable of automatically detect relevent features without human supervision. With SVMS a human would have to manually create the features. In this specific example I had trouble making out clear differences between the images in the dataset which is why svm might be problematic.\n",
        "\n",
        "3) using the torchstat we that for InceptionV3 the total memory is 36.65MB and for ResNet50 the total memory is 144.05MB. Meaning 32mb storage is not enough."
      ],
      "metadata": {
        "id": "mcIuatctITLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4)\n",
        "\n",
        "#inceptionV3 calculations:\n",
        "\n",
        "precision = tp1 / (tp1 + fp1)\n",
        "print(f'The precision is: {precision}')\n",
        "print(f'The inceptionV3 model has a {precision * 100}% chance the model will accuratly predict sars-cov3')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdiIQU-TYWeC",
        "outputId": "98c729a0-a0c8-47f1-ba97-21ec675725ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The precision is: 1.0\n",
            "The inceptionV3 model has a 100.0% chance the model will accuratly predict sars-cov3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5) This data can't represent all of canada. Canada has a population of 38M and our dataset can't represent a population that big. Our dataset may have bias frin the location it came from and may over represent groups. If it was representitive the calculation is as below."
      ],
      "metadata": {
        "id": "Cxs6qNYzd1hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5)\n",
        "\n",
        "recall = tp1 / (tp1 + fn1)\n",
        "infected = recall * 38000000\n",
        "cost = infected * 13\n",
        "\n",
        "print(recall)\n",
        "print(f'there is {infected} people infected with sars-cov3')\n",
        "print(f'it will cost ${cost} to vaccinate them')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaiAUZIcflpm",
        "outputId": "9ed1469d-ec5d-40bc-a5b1-9bedc0bf55e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9983179142136249\n",
            "there is 37936080.74011774 people infected with sars-cov3\n",
            "it will cost $493169049.62153065 to vaccinate them\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determines the number of trainable parameters in your best model\n",
        "from torchstat import stat\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "#convert to cpu\n",
        "inceptionV3Model.cpu()\n",
        "ResNet50Model.cpu()\n",
        "\n",
        "#taken from github\n",
        "stat(inceptionV3Model, (3, 224, 224))\n",
        "print('-------------------------------------------------------------------------------')\n",
        "stat(ResNet50Model, (3, 224, 224))\n"
      ],
      "metadata": {
        "id": "CozarykaTi5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1d1d04-a232-4710-9584-9cdd7da7cdd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
            "[Flops]: AdaptiveAvgPool2d is not supported!\n",
            "[Memory]: AdaptiveAvgPool2d is not supported!\n",
            "[MAdd]: Flatten is not supported!\n",
            "[Flops]: Flatten is not supported!\n",
            "[Memory]: Flatten is not supported!\n",
            "                         module name   input shape  output shape      params memory(MB)             MAdd            Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
            "0                 Conv2d_1a_3x3.conv     3 224 224    32 111 111       864.0       1.50     20,896,416.0     10,645,344.0    605568.0    1577088.0       2.46%    2182656.0\n",
            "1                   Conv2d_1a_3x3.bn    32 111 111    32 111 111        64.0       1.50      1,577,088.0        788,544.0   1577344.0    1577088.0       0.59%    3154432.0\n",
            "2                 Conv2d_2a_3x3.conv    32 111 111    32 109 109      9216.0       1.45    218,610,400.0    109,495,296.0   1613952.0    1520768.0       2.54%    3134720.0\n",
            "3                   Conv2d_2a_3x3.bn    32 109 109    32 109 109        64.0       1.45      1,520,768.0        760,384.0   1521024.0    1520768.0       0.17%    3041792.0\n",
            "4                 Conv2d_2b_3x3.conv    32 109 109    64 109 109     18432.0       2.90    437,220,800.0    218,990,592.0   1594496.0    3041536.0       4.49%    4636032.0\n",
            "5                   Conv2d_2b_3x3.bn    64 109 109    64 109 109       128.0       2.90      3,041,536.0      1,520,768.0   3042048.0    3041536.0       0.70%    6083584.0\n",
            "6                              Pool1    64 109 109    64  54  54         0.0       0.71      1,492,992.0        760,384.0   3041536.0     746496.0       1.99%    3788032.0\n",
            "7                 Conv2d_3b_1x1.conv    64  54  54    80  54  54      5120.0       0.89     29,626,560.0     14,929,920.0    766976.0     933120.0       1.39%    1700096.0\n",
            "8                   Conv2d_3b_1x1.bn    80  54  54    80  54  54       160.0       0.89        933,120.0        466,560.0    933760.0     933120.0       0.31%    1866880.0\n",
            "9                 Conv2d_4a_3x3.conv    80  54  54   192  52  52    138240.0       1.98    747,082,752.0    373,800,960.0   1486080.0    2076672.0       4.49%    3562752.0\n",
            "10                  Conv2d_4a_3x3.bn   192  52  52   192  52  52       384.0       1.98      2,076,672.0      1,038,336.0   2078208.0    2076672.0       0.51%    4154880.0\n",
            "11                             Pool2   192  52  52   192  25  25         0.0       0.46        960,000.0        519,168.0   2076672.0     480000.0       1.12%    2556672.0\n",
            "12           Mixed_5b.branch1x1.conv   192  25  25    64  25  25     12288.0       0.15     15,320,000.0      7,680,000.0    529152.0     160000.0       1.25%     689152.0\n",
            "13             Mixed_5b.branch1x1.bn    64  25  25    64  25  25       128.0       0.15        160,000.0         80,000.0    160512.0     160000.0       0.09%     320512.0\n",
            "14         Mixed_5b.branch5x5_1.conv   192  25  25    48  25  25      9216.0       0.11     11,490,000.0      5,760,000.0    516864.0     120000.0       1.11%     636864.0\n",
            "15           Mixed_5b.branch5x5_1.bn    48  25  25    48  25  25        96.0       0.11        120,000.0         60,000.0    120384.0     120000.0       0.08%     240384.0\n",
            "16         Mixed_5b.branch5x5_2.conv    48  25  25    64  25  25     76800.0       0.15     95,960,000.0     48,000,000.0    427200.0     160000.0       1.25%     587200.0\n",
            "17           Mixed_5b.branch5x5_2.bn    64  25  25    64  25  25       128.0       0.15        160,000.0         80,000.0    160512.0     160000.0       0.08%     320512.0\n",
            "18      Mixed_5b.branch3x3dbl_1.conv   192  25  25    64  25  25     12288.0       0.15     15,320,000.0      7,680,000.0    529152.0     160000.0       0.37%     689152.0\n",
            "19        Mixed_5b.branch3x3dbl_1.bn    64  25  25    64  25  25       128.0       0.15        160,000.0         80,000.0    160512.0     160000.0       0.08%     320512.0\n",
            "20      Mixed_5b.branch3x3dbl_2.conv    64  25  25    96  25  25     55296.0       0.23     69,060,000.0     34,560,000.0    381184.0     240000.0       1.09%     621184.0\n",
            "21        Mixed_5b.branch3x3dbl_2.bn    96  25  25    96  25  25       192.0       0.23        240,000.0        120,000.0    240768.0     240000.0       0.13%     480768.0\n",
            "22      Mixed_5b.branch3x3dbl_3.conv    96  25  25    96  25  25     82944.0       0.23    103,620,000.0     51,840,000.0    571776.0     240000.0       1.11%     811776.0\n",
            "23        Mixed_5b.branch3x3dbl_3.bn    96  25  25    96  25  25       192.0       0.23        240,000.0        120,000.0    240768.0     240000.0       0.09%     480768.0\n",
            "24         Mixed_5b.branch_pool.conv   192  25  25    32  25  25      6144.0       0.08      7,660,000.0      3,840,000.0    504576.0      80000.0       0.80%     584576.0\n",
            "25           Mixed_5b.branch_pool.bn    32  25  25    32  25  25        64.0       0.08         80,000.0         40,000.0     80256.0      80000.0       0.07%     160256.0\n",
            "26           Mixed_5c.branch1x1.conv   256  25  25    64  25  25     16384.0       0.15     20,440,000.0     10,240,000.0    705536.0     160000.0       1.22%     865536.0\n",
            "27             Mixed_5c.branch1x1.bn    64  25  25    64  25  25       128.0       0.15        160,000.0         80,000.0    160512.0     160000.0       0.08%     320512.0\n",
            "28         Mixed_5c.branch5x5_1.conv   256  25  25    48  25  25     12288.0       0.11     15,330,000.0      7,680,000.0    689152.0     120000.0       1.01%     809152.0\n",
            "29           Mixed_5c.branch5x5_1.bn    48  25  25    48  25  25        96.0       0.11        120,000.0         60,000.0    120384.0     120000.0       0.08%     240384.0\n",
            "30         Mixed_5c.branch5x5_2.conv    48  25  25    64  25  25     76800.0       0.15     95,960,000.0     48,000,000.0    427200.0     160000.0       0.64%     587200.0\n",
            "31           Mixed_5c.branch5x5_2.bn    64  25  25    64  25  25       128.0       0.15        160,000.0         80,000.0    160512.0     160000.0       0.08%     320512.0\n",
            "32      Mixed_5c.branch3x3dbl_1.conv   256  25  25    64  25  25     16384.0       0.15     20,440,000.0     10,240,000.0    705536.0     160000.0       0.35%     865536.0\n",
            "33        Mixed_5c.branch3x3dbl_1.bn    64  25  25    64  25  25       128.0       0.15        160,000.0         80,000.0    160512.0     160000.0       0.09%     320512.0\n",
            "34      Mixed_5c.branch3x3dbl_2.conv    64  25  25    96  25  25     55296.0       0.23     69,060,000.0     34,560,000.0    381184.0     240000.0       0.57%     621184.0\n",
            "35        Mixed_5c.branch3x3dbl_2.bn    96  25  25    96  25  25       192.0       0.23        240,000.0        120,000.0    240768.0     240000.0       0.07%     480768.0\n",
            "36      Mixed_5c.branch3x3dbl_3.conv    96  25  25    96  25  25     82944.0       0.23    103,620,000.0     51,840,000.0    571776.0     240000.0       0.78%     811776.0\n",
            "37        Mixed_5c.branch3x3dbl_3.bn    96  25  25    96  25  25       192.0       0.23        240,000.0        120,000.0    240768.0     240000.0       0.08%     480768.0\n",
            "38         Mixed_5c.branch_pool.conv   256  25  25    64  25  25     16384.0       0.15     20,440,000.0     10,240,000.0    705536.0     160000.0       0.50%     865536.0\n",
            "39           Mixed_5c.branch_pool.bn    64  25  25    64  25  25       128.0       0.15        160,000.0         80,000.0    160512.0     160000.0       0.08%     320512.0\n",
            "40           Mixed_5d.branch1x1.conv   288  25  25    64  25  25     18432.0       0.15     23,000,000.0     11,520,000.0    793728.0     160000.0       1.29%     953728.0\n",
            "41             Mixed_5d.branch1x1.bn    64  25  25    64  25  25       128.0       0.15        160,000.0         80,000.0    160512.0     160000.0       0.08%     320512.0\n",
            "42         Mixed_5d.branch5x5_1.conv   288  25  25    48  25  25     13824.0       0.11     17,250,000.0      8,640,000.0    775296.0     120000.0       1.17%     895296.0\n",
            "43           Mixed_5d.branch5x5_1.bn    48  25  25    48  25  25        96.0       0.11        120,000.0         60,000.0    120384.0     120000.0       0.09%     240384.0\n",
            "44         Mixed_5d.branch5x5_2.conv    48  25  25    64  25  25     76800.0       0.15     95,960,000.0     48,000,000.0    427200.0     160000.0       0.61%     587200.0\n",
            "45           Mixed_5d.branch5x5_2.bn    64  25  25    64  25  25       128.0       0.15        160,000.0         80,000.0    160512.0     160000.0       0.08%     320512.0\n",
            "46      Mixed_5d.branch3x3dbl_1.conv   288  25  25    64  25  25     18432.0       0.15     23,000,000.0     11,520,000.0    793728.0     160000.0       0.53%     953728.0\n",
            "47        Mixed_5d.branch3x3dbl_1.bn    64  25  25    64  25  25       128.0       0.15        160,000.0         80,000.0    160512.0     160000.0       0.08%     320512.0\n",
            "48      Mixed_5d.branch3x3dbl_2.conv    64  25  25    96  25  25     55296.0       0.23     69,060,000.0     34,560,000.0    381184.0     240000.0       0.56%     621184.0\n",
            "49        Mixed_5d.branch3x3dbl_2.bn    96  25  25    96  25  25       192.0       0.23        240,000.0        120,000.0    240768.0     240000.0       0.14%     480768.0\n",
            "50      Mixed_5d.branch3x3dbl_3.conv    96  25  25    96  25  25     82944.0       0.23    103,620,000.0     51,840,000.0    571776.0     240000.0       0.81%     811776.0\n",
            "51        Mixed_5d.branch3x3dbl_3.bn    96  25  25    96  25  25       192.0       0.23        240,000.0        120,000.0    240768.0     240000.0       0.08%     480768.0\n",
            "52         Mixed_5d.branch_pool.conv   288  25  25    64  25  25     18432.0       0.15     23,000,000.0     11,520,000.0    793728.0     160000.0       0.50%     953728.0\n",
            "53           Mixed_5d.branch_pool.bn    64  25  25    64  25  25       128.0       0.15        160,000.0         80,000.0    160512.0     160000.0       0.08%     320512.0\n",
            "54           Mixed_6a.branch3x3.conv   288  25  25   384  12  12    995328.0       0.21    286,599,168.0    143,327,232.0   4701312.0     221184.0       2.79%    4922496.0\n",
            "55             Mixed_6a.branch3x3.bn   384  12  12   384  12  12       768.0       0.21        221,184.0        110,592.0    224256.0     221184.0       0.07%     445440.0\n",
            "56      Mixed_6a.branch3x3dbl_1.conv   288  25  25    64  25  25     18432.0       0.15     23,000,000.0     11,520,000.0    793728.0     160000.0       0.44%     953728.0\n",
            "57        Mixed_6a.branch3x3dbl_1.bn    64  25  25    64  25  25       128.0       0.15        160,000.0         80,000.0    160512.0     160000.0       0.09%     320512.0\n",
            "58      Mixed_6a.branch3x3dbl_2.conv    64  25  25    96  25  25     55296.0       0.23     69,060,000.0     34,560,000.0    381184.0     240000.0       0.58%     621184.0\n",
            "59        Mixed_6a.branch3x3dbl_2.bn    96  25  25    96  25  25       192.0       0.23        240,000.0        120,000.0    240768.0     240000.0       0.07%     480768.0\n",
            "60      Mixed_6a.branch3x3dbl_3.conv    96  25  25    96  12  12     82944.0       0.05     23,874,048.0     11,943,936.0    571776.0      55296.0       0.78%     627072.0\n",
            "61        Mixed_6a.branch3x3dbl_3.bn    96  12  12    96  12  12       192.0       0.05         55,296.0         27,648.0     56064.0      55296.0       0.07%     111360.0\n",
            "62           Mixed_6b.branch1x1.conv   768  12  12   192  12  12    147456.0       0.11     42,439,680.0     21,233,664.0   1032192.0     110592.0       1.13%    1142784.0\n",
            "63             Mixed_6b.branch1x1.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "64         Mixed_6b.branch7x7_1.conv   768  12  12   128  12  12     98304.0       0.07     28,293,120.0     14,155,776.0    835584.0      73728.0       1.03%     909312.0\n",
            "65           Mixed_6b.branch7x7_1.bn   128  12  12   128  12  12       256.0       0.07         73,728.0         36,864.0     74752.0      73728.0       0.07%     148480.0\n",
            "66         Mixed_6b.branch7x7_2.conv   128  12  12   128  12  12    114688.0       0.07     33,011,712.0     16,515,072.0    532480.0      73728.0       0.64%     606208.0\n",
            "67           Mixed_6b.branch7x7_2.bn   128  12  12   128  12  12       256.0       0.07         73,728.0         36,864.0     74752.0      73728.0       0.08%     148480.0\n",
            "68         Mixed_6b.branch7x7_3.conv   128  12  12   192  12  12    172032.0       0.11     49,517,568.0     24,772,608.0    761856.0     110592.0       0.30%     872448.0\n",
            "69           Mixed_6b.branch7x7_3.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "70      Mixed_6b.branch7x7dbl_1.conv   768  12  12   128  12  12     98304.0       0.07     28,293,120.0     14,155,776.0    835584.0      73728.0       0.45%     909312.0\n",
            "71        Mixed_6b.branch7x7dbl_1.bn   128  12  12   128  12  12       256.0       0.07         73,728.0         36,864.0     74752.0      73728.0       0.08%     148480.0\n",
            "72      Mixed_6b.branch7x7dbl_2.conv   128  12  12   128  12  12    114688.0       0.07     33,011,712.0     16,515,072.0    532480.0      73728.0       0.24%     606208.0\n",
            "73        Mixed_6b.branch7x7dbl_2.bn   128  12  12   128  12  12       256.0       0.07         73,728.0         36,864.0     74752.0      73728.0       0.07%     148480.0\n",
            "74      Mixed_6b.branch7x7dbl_3.conv   128  12  12   128  12  12    114688.0       0.07     33,011,712.0     16,515,072.0    532480.0      73728.0       0.48%     606208.0\n",
            "75        Mixed_6b.branch7x7dbl_3.bn   128  12  12   128  12  12       256.0       0.07         73,728.0         36,864.0     74752.0      73728.0       0.07%     148480.0\n",
            "76      Mixed_6b.branch7x7dbl_4.conv   128  12  12   128  12  12    114688.0       0.07     33,011,712.0     16,515,072.0    532480.0      73728.0       0.47%     606208.0\n",
            "77        Mixed_6b.branch7x7dbl_4.bn   128  12  12   128  12  12       256.0       0.07         73,728.0         36,864.0     74752.0      73728.0       0.08%     148480.0\n",
            "78      Mixed_6b.branch7x7dbl_5.conv   128  12  12   192  12  12    172032.0       0.11     49,517,568.0     24,772,608.0    761856.0     110592.0       0.41%     872448.0\n",
            "79        Mixed_6b.branch7x7dbl_5.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "80         Mixed_6b.branch_pool.conv   768  12  12   192  12  12    147456.0       0.11     42,439,680.0     21,233,664.0   1032192.0     110592.0       0.56%    1142784.0\n",
            "81           Mixed_6b.branch_pool.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.10%     222720.0\n",
            "82           Mixed_6c.branch1x1.conv   768  12  12   192  12  12    147456.0       0.11     42,439,680.0     21,233,664.0   1032192.0     110592.0       0.53%    1142784.0\n",
            "83             Mixed_6c.branch1x1.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.09%     222720.0\n",
            "84         Mixed_6c.branch7x7_1.conv   768  12  12   160  12  12    122880.0       0.09     35,366,400.0     17,694,720.0    933888.0      92160.0       1.17%    1026048.0\n",
            "85           Mixed_6c.branch7x7_1.bn   160  12  12   160  12  12       320.0       0.09         92,160.0         46,080.0     93440.0      92160.0       0.09%     185600.0\n",
            "86         Mixed_6c.branch7x7_2.conv   160  12  12   160  12  12    179200.0       0.09     51,586,560.0     25,804,800.0    808960.0      92160.0       1.07%     901120.0\n",
            "87           Mixed_6c.branch7x7_2.bn   160  12  12   160  12  12       320.0       0.09         92,160.0         46,080.0     93440.0      92160.0       0.08%     185600.0\n",
            "88         Mixed_6c.branch7x7_3.conv   160  12  12   192  12  12    215040.0       0.11     61,903,872.0     30,965,760.0    952320.0     110592.0       0.90%    1062912.0\n",
            "89           Mixed_6c.branch7x7_3.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "90      Mixed_6c.branch7x7dbl_1.conv   768  12  12   160  12  12    122880.0       0.09     35,366,400.0     17,694,720.0    933888.0      92160.0       0.48%    1026048.0\n",
            "91        Mixed_6c.branch7x7dbl_1.bn   160  12  12   160  12  12       320.0       0.09         92,160.0         46,080.0     93440.0      92160.0       0.07%     185600.0\n",
            "92      Mixed_6c.branch7x7dbl_2.conv   160  12  12   160  12  12    179200.0       0.09     51,586,560.0     25,804,800.0    808960.0      92160.0       0.81%     901120.0\n",
            "93        Mixed_6c.branch7x7dbl_2.bn   160  12  12   160  12  12       320.0       0.09         92,160.0         46,080.0     93440.0      92160.0       0.08%     185600.0\n",
            "94      Mixed_6c.branch7x7dbl_3.conv   160  12  12   160  12  12    179200.0       0.09     51,586,560.0     25,804,800.0    808960.0      92160.0       0.54%     901120.0\n",
            "95        Mixed_6c.branch7x7dbl_3.bn   160  12  12   160  12  12       320.0       0.09         92,160.0         46,080.0     93440.0      92160.0       0.08%     185600.0\n",
            "96      Mixed_6c.branch7x7dbl_4.conv   160  12  12   160  12  12    179200.0       0.09     51,586,560.0     25,804,800.0    808960.0      92160.0       0.42%     901120.0\n",
            "97        Mixed_6c.branch7x7dbl_4.bn   160  12  12   160  12  12       320.0       0.09         92,160.0         46,080.0     93440.0      92160.0       0.08%     185600.0\n",
            "98      Mixed_6c.branch7x7dbl_5.conv   160  12  12   192  12  12    215040.0       0.11     61,903,872.0     30,965,760.0    952320.0     110592.0       1.11%    1062912.0\n",
            "99        Mixed_6c.branch7x7dbl_5.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.07%     222720.0\n",
            "100        Mixed_6c.branch_pool.conv   768  12  12   192  12  12    147456.0       0.11     42,439,680.0     21,233,664.0   1032192.0     110592.0       0.65%    1142784.0\n",
            "101          Mixed_6c.branch_pool.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "102          Mixed_6d.branch1x1.conv   768  12  12   192  12  12    147456.0       0.11     42,439,680.0     21,233,664.0   1032192.0     110592.0       0.65%    1142784.0\n",
            "103            Mixed_6d.branch1x1.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "104        Mixed_6d.branch7x7_1.conv   768  12  12   160  12  12    122880.0       0.09     35,366,400.0     17,694,720.0    933888.0      92160.0       0.53%    1026048.0\n",
            "105          Mixed_6d.branch7x7_1.bn   160  12  12   160  12  12       320.0       0.09         92,160.0         46,080.0     93440.0      92160.0       0.07%     185600.0\n",
            "106        Mixed_6d.branch7x7_2.conv   160  12  12   160  12  12    179200.0       0.09     51,586,560.0     25,804,800.0    808960.0      92160.0       0.55%     901120.0\n",
            "107          Mixed_6d.branch7x7_2.bn   160  12  12   160  12  12       320.0       0.09         92,160.0         46,080.0     93440.0      92160.0       0.08%     185600.0\n",
            "108        Mixed_6d.branch7x7_3.conv   160  12  12   192  12  12    215040.0       0.11     61,903,872.0     30,965,760.0    952320.0     110592.0       0.67%    1062912.0\n",
            "109          Mixed_6d.branch7x7_3.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.25%     222720.0\n",
            "110     Mixed_6d.branch7x7dbl_1.conv   768  12  12   160  12  12    122880.0       0.09     35,366,400.0     17,694,720.0    933888.0      92160.0       0.58%    1026048.0\n",
            "111       Mixed_6d.branch7x7dbl_1.bn   160  12  12   160  12  12       320.0       0.09         92,160.0         46,080.0     93440.0      92160.0       0.09%     185600.0\n",
            "112     Mixed_6d.branch7x7dbl_2.conv   160  12  12   160  12  12    179200.0       0.09     51,586,560.0     25,804,800.0    808960.0      92160.0       0.62%     901120.0\n",
            "113       Mixed_6d.branch7x7dbl_2.bn   160  12  12   160  12  12       320.0       0.09         92,160.0         46,080.0     93440.0      92160.0       0.08%     185600.0\n",
            "114     Mixed_6d.branch7x7dbl_3.conv   160  12  12   160  12  12    179200.0       0.09     51,586,560.0     25,804,800.0    808960.0      92160.0       0.57%     901120.0\n",
            "115       Mixed_6d.branch7x7dbl_3.bn   160  12  12   160  12  12       320.0       0.09         92,160.0         46,080.0     93440.0      92160.0       0.08%     185600.0\n",
            "116     Mixed_6d.branch7x7dbl_4.conv   160  12  12   160  12  12    179200.0       0.09     51,586,560.0     25,804,800.0    808960.0      92160.0       0.68%     901120.0\n",
            "117       Mixed_6d.branch7x7dbl_4.bn   160  12  12   160  12  12       320.0       0.09         92,160.0         46,080.0     93440.0      92160.0       0.08%     185600.0\n",
            "118     Mixed_6d.branch7x7dbl_5.conv   160  12  12   192  12  12    215040.0       0.11     61,903,872.0     30,965,760.0    952320.0     110592.0       0.66%    1062912.0\n",
            "119       Mixed_6d.branch7x7dbl_5.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "120        Mixed_6d.branch_pool.conv   768  12  12   192  12  12    147456.0       0.11     42,439,680.0     21,233,664.0   1032192.0     110592.0       0.64%    1142784.0\n",
            "121          Mixed_6d.branch_pool.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.09%     222720.0\n",
            "122          Mixed_6e.branch1x1.conv   768  12  12   192  12  12    147456.0       0.11     42,439,680.0     21,233,664.0   1032192.0     110592.0       0.69%    1142784.0\n",
            "123            Mixed_6e.branch1x1.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.07%     222720.0\n",
            "124        Mixed_6e.branch7x7_1.conv   768  12  12   192  12  12    147456.0       0.11     42,439,680.0     21,233,664.0   1032192.0     110592.0       0.62%    1142784.0\n",
            "125          Mixed_6e.branch7x7_1.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "126        Mixed_6e.branch7x7_2.conv   192  12  12   192  12  12    258048.0       0.11     74,290,176.0     37,158,912.0   1142784.0     110592.0       1.34%    1253376.0\n",
            "127          Mixed_6e.branch7x7_2.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.09%     222720.0\n",
            "128        Mixed_6e.branch7x7_3.conv   192  12  12   192  12  12    258048.0       0.11     74,290,176.0     37,158,912.0   1142784.0     110592.0       1.03%    1253376.0\n",
            "129          Mixed_6e.branch7x7_3.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "130     Mixed_6e.branch7x7dbl_1.conv   768  12  12   192  12  12    147456.0       0.11     42,439,680.0     21,233,664.0   1032192.0     110592.0       0.49%    1142784.0\n",
            "131       Mixed_6e.branch7x7dbl_1.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.11%     222720.0\n",
            "132     Mixed_6e.branch7x7dbl_2.conv   192  12  12   192  12  12    258048.0       0.11     74,290,176.0     37,158,912.0   1142784.0     110592.0       0.77%    1253376.0\n",
            "133       Mixed_6e.branch7x7dbl_2.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.11%     222720.0\n",
            "134     Mixed_6e.branch7x7dbl_3.conv   192  12  12   192  12  12    258048.0       0.11     74,290,176.0     37,158,912.0   1142784.0     110592.0       0.82%    1253376.0\n",
            "135       Mixed_6e.branch7x7dbl_3.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.12%     222720.0\n",
            "136     Mixed_6e.branch7x7dbl_4.conv   192  12  12   192  12  12    258048.0       0.11     74,290,176.0     37,158,912.0   1142784.0     110592.0       0.82%    1253376.0\n",
            "137       Mixed_6e.branch7x7dbl_4.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.12%     222720.0\n",
            "138     Mixed_6e.branch7x7dbl_5.conv   192  12  12   192  12  12    258048.0       0.11     74,290,176.0     37,158,912.0   1142784.0     110592.0       0.60%    1253376.0\n",
            "139       Mixed_6e.branch7x7dbl_5.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "140        Mixed_6e.branch_pool.conv   768  12  12   192  12  12    147456.0       0.11     42,439,680.0     21,233,664.0   1032192.0     110592.0       0.55%    1142784.0\n",
            "141          Mixed_6e.branch_pool.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "142        Mixed_7a.branch3x3_1.conv   768  12  12   192  12  12    147456.0       0.11     42,439,680.0     21,233,664.0   1032192.0     110592.0       0.61%    1142784.0\n",
            "143          Mixed_7a.branch3x3_1.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.11%     222720.0\n",
            "144        Mixed_7a.branch3x3_2.conv   192  12  12   320   5   5    552960.0       0.03     27,640,000.0     13,824,000.0   2322432.0      32000.0       1.42%    2354432.0\n",
            "145          Mixed_7a.branch3x3_2.bn   320   5   5   320   5   5       640.0       0.03         32,000.0         16,000.0     34560.0      32000.0       0.08%      66560.0\n",
            "146      Mixed_7a.branch7x7x3_1.conv   768  12  12   192  12  12    147456.0       0.11     42,439,680.0     21,233,664.0   1032192.0     110592.0       0.51%    1142784.0\n",
            "147        Mixed_7a.branch7x7x3_1.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.07%     222720.0\n",
            "148      Mixed_7a.branch7x7x3_2.conv   192  12  12   192  12  12    258048.0       0.11     74,290,176.0     37,158,912.0   1142784.0     110592.0       0.67%    1253376.0\n",
            "149        Mixed_7a.branch7x7x3_2.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "150      Mixed_7a.branch7x7x3_3.conv   192  12  12   192  12  12    258048.0       0.11     74,290,176.0     37,158,912.0   1142784.0     110592.0       0.69%    1253376.0\n",
            "151        Mixed_7a.branch7x7x3_3.bn   192  12  12   192  12  12       384.0       0.11        110,592.0         55,296.0    112128.0     110592.0       0.08%     222720.0\n",
            "152      Mixed_7a.branch7x7x3_4.conv   192  12  12   192   5   5    331776.0       0.02     16,584,000.0      8,294,400.0   1437696.0      19200.0       1.01%    1456896.0\n",
            "153        Mixed_7a.branch7x7x3_4.bn   192   5   5   192   5   5       384.0       0.02         19,200.0          9,600.0     20736.0      19200.0       0.09%      39936.0\n",
            "154          Mixed_7b.branch1x1.conv  1280   5   5   320   5   5    409600.0       0.03     20,472,000.0     10,240,000.0   1766400.0      32000.0       1.47%    1798400.0\n",
            "155            Mixed_7b.branch1x1.bn   320   5   5   320   5   5       640.0       0.03         32,000.0         16,000.0     34560.0      32000.0       0.08%      66560.0\n",
            "156        Mixed_7b.branch3x3_1.conv  1280   5   5   384   5   5    491520.0       0.04     24,566,400.0     12,288,000.0   2094080.0      38400.0       1.62%    2132480.0\n",
            "157          Mixed_7b.branch3x3_1.bn   384   5   5   384   5   5       768.0       0.04         38,400.0         19,200.0     41472.0      38400.0       0.08%      79872.0\n",
            "158       Mixed_7b.branch3x3_2a.conv   384   5   5   384   5   5    442368.0       0.04     22,108,800.0     11,059,200.0   1807872.0      38400.0       0.41%    1846272.0\n",
            "159         Mixed_7b.branch3x3_2a.bn   384   5   5   384   5   5       768.0       0.04         38,400.0         19,200.0     41472.0      38400.0       0.08%      79872.0\n",
            "160       Mixed_7b.branch3x3_2b.conv   384   5   5   384   5   5    442368.0       0.04     22,108,800.0     11,059,200.0   1807872.0      38400.0       0.23%    1846272.0\n",
            "161         Mixed_7b.branch3x3_2b.bn   384   5   5   384   5   5       768.0       0.04         38,400.0         19,200.0     41472.0      38400.0       0.08%      79872.0\n",
            "162     Mixed_7b.branch3x3dbl_1.conv  1280   5   5   448   5   5    573440.0       0.04     28,660,800.0     14,336,000.0   2421760.0      44800.0       1.69%    2466560.0\n",
            "163       Mixed_7b.branch3x3dbl_1.bn   448   5   5   448   5   5       896.0       0.04         44,800.0         22,400.0     48384.0      44800.0       0.10%      93184.0\n",
            "164     Mixed_7b.branch3x3dbl_2.conv   448   5   5   384   5   5   1548288.0       0.04     77,404,800.0     38,707,200.0   6237952.0      38400.0       0.73%    6276352.0\n",
            "165       Mixed_7b.branch3x3dbl_2.bn   384   5   5   384   5   5       768.0       0.04         38,400.0         19,200.0     41472.0      38400.0       0.08%      79872.0\n",
            "166    Mixed_7b.branch3x3dbl_3a.conv   384   5   5   384   5   5    442368.0       0.04     22,108,800.0     11,059,200.0   1807872.0      38400.0       0.27%    1846272.0\n",
            "167      Mixed_7b.branch3x3dbl_3a.bn   384   5   5   384   5   5       768.0       0.04         38,400.0         19,200.0     41472.0      38400.0       0.08%      79872.0\n",
            "168    Mixed_7b.branch3x3dbl_3b.conv   384   5   5   384   5   5    442368.0       0.04     22,108,800.0     11,059,200.0   1807872.0      38400.0       0.23%    1846272.0\n",
            "169      Mixed_7b.branch3x3dbl_3b.bn   384   5   5   384   5   5       768.0       0.04         38,400.0         19,200.0     41472.0      38400.0       0.08%      79872.0\n",
            "170        Mixed_7b.branch_pool.conv  1280   5   5   192   5   5    245760.0       0.02     12,283,200.0      6,144,000.0   1111040.0      19200.0       1.17%    1130240.0\n",
            "171          Mixed_7b.branch_pool.bn   192   5   5   192   5   5       384.0       0.02         19,200.0          9,600.0     20736.0      19200.0       0.08%      39936.0\n",
            "172          Mixed_7c.branch1x1.conv  2048   5   5   320   5   5    655360.0       0.03     32,760,000.0     16,384,000.0   2826240.0      32000.0       1.68%    2858240.0\n",
            "173            Mixed_7c.branch1x1.bn   320   5   5   320   5   5       640.0       0.03         32,000.0         16,000.0     34560.0      32000.0       0.08%      66560.0\n",
            "174        Mixed_7c.branch3x3_1.conv  2048   5   5   384   5   5    786432.0       0.04     39,312,000.0     19,660,800.0   3350528.0      38400.0       1.92%    3388928.0\n",
            "175          Mixed_7c.branch3x3_1.bn   384   5   5   384   5   5       768.0       0.04         38,400.0         19,200.0     41472.0      38400.0       0.12%      79872.0\n",
            "176       Mixed_7c.branch3x3_2a.conv   384   5   5   384   5   5    442368.0       0.04     22,108,800.0     11,059,200.0   1807872.0      38400.0       0.35%    1846272.0\n",
            "177         Mixed_7c.branch3x3_2a.bn   384   5   5   384   5   5       768.0       0.04         38,400.0         19,200.0     41472.0      38400.0       0.08%      79872.0\n",
            "178       Mixed_7c.branch3x3_2b.conv   384   5   5   384   5   5    442368.0       0.04     22,108,800.0     11,059,200.0   1807872.0      38400.0       0.37%    1846272.0\n",
            "179         Mixed_7c.branch3x3_2b.bn   384   5   5   384   5   5       768.0       0.04         38,400.0         19,200.0     41472.0      38400.0       0.09%      79872.0\n",
            "180     Mixed_7c.branch3x3dbl_1.conv  2048   5   5   448   5   5    917504.0       0.04     45,864,000.0     22,937,600.0   3874816.0      44800.0       1.82%    3919616.0\n",
            "181       Mixed_7c.branch3x3dbl_1.bn   448   5   5   448   5   5       896.0       0.04         44,800.0         22,400.0     48384.0      44800.0       0.08%      93184.0\n",
            "182     Mixed_7c.branch3x3dbl_2.conv   448   5   5   384   5   5   1548288.0       0.04     77,404,800.0     38,707,200.0   6237952.0      38400.0       0.76%    6276352.0\n",
            "183       Mixed_7c.branch3x3dbl_2.bn   384   5   5   384   5   5       768.0       0.04         38,400.0         19,200.0     41472.0      38400.0       0.12%      79872.0\n",
            "184    Mixed_7c.branch3x3dbl_3a.conv   384   5   5   384   5   5    442368.0       0.04     22,108,800.0     11,059,200.0   1807872.0      38400.0       0.32%    1846272.0\n",
            "185      Mixed_7c.branch3x3dbl_3a.bn   384   5   5   384   5   5       768.0       0.04         38,400.0         19,200.0     41472.0      38400.0       0.07%      79872.0\n",
            "186    Mixed_7c.branch3x3dbl_3b.conv   384   5   5   384   5   5    442368.0       0.04     22,108,800.0     11,059,200.0   1807872.0      38400.0       0.24%    1846272.0\n",
            "187      Mixed_7c.branch3x3dbl_3b.bn   384   5   5   384   5   5       768.0       0.04         38,400.0         19,200.0     41472.0      38400.0       0.09%      79872.0\n",
            "188        Mixed_7c.branch_pool.conv  2048   5   5   192   5   5    393216.0       0.02     19,656,000.0      9,830,400.0   1777664.0      19200.0       1.20%    1796864.0\n",
            "189          Mixed_7c.branch_pool.bn   192   5   5   192   5   5       384.0       0.02         19,200.0          9,600.0     20736.0      19200.0       0.08%      39936.0\n",
            "190              global_pool.flatten  2048   1   1          2048         0.0       0.01              0.0              0.0         0.0          0.0       0.32%          0.0\n",
            "191                 global_pool.pool  2048   5   5  2048   1   1         0.0       0.01              0.0              0.0         0.0          0.0       0.21%          0.0\n",
            "192                               fc          2048             3      6147.0       0.00         12,285.0          6,144.0     32780.0         12.0       0.25%      32792.0\n",
            "total                                                             21791715.0      36.65  5,688,156,525.0  2,846,455,488.0     32780.0         12.0     100.00%  177784216.0\n",
            "===========================================================================================================================================================================\n",
            "Total params: 21,791,715\n",
            "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Total memory: 36.65MB\n",
            "Total MAdd: 5.69GMAdd\n",
            "Total Flops: 2.85GFlops\n",
            "Total MemR+W: 169.55MB\n",
            "\n",
            "-------------------------------------------------------------------------------\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchstat/reporter.py:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(total_df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: Identity is not supported!\n",
            "[Flops]: Identity is not supported!\n",
            "[Memory]: Identity is not supported!\n",
            "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
            "[Flops]: AdaptiveAvgPool2d is not supported!\n",
            "[Memory]: AdaptiveAvgPool2d is not supported!\n",
            "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
            "[Flops]: AdaptiveAvgPool2d is not supported!\n",
            "[Memory]: AdaptiveAvgPool2d is not supported!\n",
            "[MAdd]: Flatten is not supported!\n",
            "[Flops]: Flatten is not supported!\n",
            "[Memory]: Flatten is not supported!\n",
            "[MAdd]: Flatten is not supported!\n",
            "[Flops]: Flatten is not supported!\n",
            "[Memory]: Flatten is not supported!\n",
            "                 module name   input shape  output shape      params memory(MB)             MAdd            Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
            "0                    conv1.0     3 224 224    32 112 112       864.0       1.53     21,274,624.0     10,838,016.0    605568.0    1605632.0       1.04%    2211200.0\n",
            "1                    conv1.1    32 112 112    32 112 112        64.0       1.53      1,605,632.0        802,816.0   1605888.0    1605632.0       0.39%    3211520.0\n",
            "2                    conv1.2    32 112 112    32 112 112         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.07%    3211264.0\n",
            "3                    conv1.3    32 112 112    32 112 112      9216.0       1.53    230,809,600.0    115,605,504.0   1642496.0    1605632.0       2.01%    3248128.0\n",
            "4                    conv1.4    32 112 112    32 112 112        64.0       1.53      1,605,632.0        802,816.0   1605888.0    1605632.0       0.27%    3211520.0\n",
            "5                    conv1.5    32 112 112    32 112 112         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.06%    3211264.0\n",
            "6                    conv1.6    32 112 112    64 112 112     18432.0       3.06    461,619,200.0    231,211,008.0   1679360.0    3211264.0       2.61%    4890624.0\n",
            "7                        bn1    64 112 112    64 112 112       128.0       3.06      3,211,264.0      1,605,632.0   3211776.0    3211264.0       0.46%    6423040.0\n",
            "8                       act1    64 112 112    64 112 112         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.08%    6422528.0\n",
            "9                    maxpool    64 112 112    64  56  56         0.0       0.77      1,605,632.0        802,816.0   3211264.0     802816.0       1.63%    4014080.0\n",
            "10            layer1.0.conv1    64  56  56    64  56  56      4096.0       0.77     25,489,408.0     12,845,056.0    819200.0     802816.0       1.04%    1622016.0\n",
            "11              layer1.0.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.24%    1606144.0\n",
            "12             layer1.0.act1    64  56  56    64  56  56         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.05%    1605632.0\n",
            "13            layer1.0.conv2    64  56  56    64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       1.75%    1753088.0\n",
            "14              layer1.0.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.21%    1606144.0\n",
            "15       layer1.0.drop_block    64  56  56    64  56  56         0.0       0.77              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "16             layer1.0.act2    64  56  56    64  56  56         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.05%    1605632.0\n",
            "17               layer1.0.aa    64  56  56    64  56  56         0.0       0.77              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "18            layer1.0.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       2.03%    4079616.0\n",
            "19              layer1.0.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.66%    6424576.0\n",
            "20             layer1.0.act3   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.10%    6422528.0\n",
            "21     layer1.0.downsample.0    64  56  56    64  56  56         0.0       0.77              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "22     layer1.0.downsample.1    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.64%    4079616.0\n",
            "23     layer1.0.downsample.2   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.76%    6424576.0\n",
            "24            layer1.1.conv1   256  56  56    64  56  56     16384.0       0.77    102,559,744.0     51,380,224.0   3276800.0     802816.0       1.46%    4079616.0\n",
            "25              layer1.1.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.30%    1606144.0\n",
            "26             layer1.1.act1    64  56  56    64  56  56         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0\n",
            "27            layer1.1.conv2    64  56  56    64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       1.26%    1753088.0\n",
            "28              layer1.1.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.32%    1606144.0\n",
            "29       layer1.1.drop_block    64  56  56    64  56  56         0.0       0.77              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "30             layer1.1.act2    64  56  56    64  56  56         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.05%    1605632.0\n",
            "31               layer1.1.aa    64  56  56    64  56  56         0.0       0.77              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "32            layer1.1.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.27%    4079616.0\n",
            "33              layer1.1.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.65%    6424576.0\n",
            "34             layer1.1.act3   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.07%    6422528.0\n",
            "35            layer1.2.conv1   256  56  56    64  56  56     16384.0       0.77    102,559,744.0     51,380,224.0   3276800.0     802816.0       1.20%    4079616.0\n",
            "36              layer1.2.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.25%    1606144.0\n",
            "37             layer1.2.act1    64  56  56    64  56  56         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0\n",
            "38            layer1.2.conv2    64  56  56    64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       1.08%    1753088.0\n",
            "39              layer1.2.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.20%    1606144.0\n",
            "40       layer1.2.drop_block    64  56  56    64  56  56         0.0       0.77              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "41             layer1.2.act2    64  56  56    64  56  56         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.05%    1605632.0\n",
            "42               layer1.2.aa    64  56  56    64  56  56         0.0       0.77              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "43            layer1.2.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.22%    4079616.0\n",
            "44              layer1.2.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.70%    6424576.0\n",
            "45             layer1.2.act3   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.07%    6422528.0\n",
            "46            layer2.0.conv1   256  56  56   128  56  56     32768.0       1.53    205,119,488.0    102,760,448.0   3342336.0    1605632.0       2.03%    4947968.0\n",
            "47              layer2.0.bn1   128  56  56   128  56  56       256.0       1.53      1,605,632.0        802,816.0   1606656.0    1605632.0       0.27%    3212288.0\n",
            "48             layer2.0.act1   128  56  56   128  56  56         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.06%    3211264.0\n",
            "49            layer2.0.conv2   128  56  56   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0   2195456.0     401408.0       1.67%    2596864.0\n",
            "50              layer2.0.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.19%     803840.0\n",
            "51       layer2.0.drop_block   128  28  28   128  28  28         0.0       0.38              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "52             layer2.0.act2   128  28  28   128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.05%     802816.0\n",
            "53               layer2.0.aa   128  28  28   128  28  28         0.0       0.38              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "54            layer2.0.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.28%    2269184.0\n",
            "55              layer2.0.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.26%    3215360.0\n",
            "56             layer2.0.act3   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.04%    3211264.0\n",
            "57     layer2.0.downsample.0   256  56  56   256  28  28         0.0       0.77        802,816.0        802,816.0   3211264.0     802816.0       0.96%    4014080.0\n",
            "58     layer2.0.downsample.1   256  28  28   512  28  28    131072.0       1.53    205,119,488.0    102,760,448.0   1327104.0    1605632.0       1.40%    2932736.0\n",
            "59     layer2.0.downsample.2   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.51%    3215360.0\n",
            "60            layer2.1.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       1.18%    2269184.0\n",
            "61              layer2.1.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.22%     803840.0\n",
            "62             layer2.1.act1   128  28  28   128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.04%     802816.0\n",
            "63            layer2.1.conv2   128  28  28   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       2.03%    1392640.0\n",
            "64              layer2.1.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.25%     803840.0\n",
            "65       layer2.1.drop_block   128  28  28   128  28  28         0.0       0.38              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "66             layer2.1.act2   128  28  28   128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.05%     802816.0\n",
            "67               layer2.1.aa   128  28  28   128  28  28         0.0       0.38              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "68            layer2.1.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.04%    2269184.0\n",
            "69              layer2.1.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.64%    3215360.0\n",
            "70             layer2.1.act3   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.05%    3211264.0\n",
            "71            layer2.2.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       1.05%    2269184.0\n",
            "72              layer2.2.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.24%     803840.0\n",
            "73             layer2.2.act1   128  28  28   128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.04%     802816.0\n",
            "74            layer2.2.conv2   128  28  28   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       1.23%    1392640.0\n",
            "75              layer2.2.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.20%     803840.0\n",
            "76       layer2.2.drop_block   128  28  28   128  28  28         0.0       0.38              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "77             layer2.2.act2   128  28  28   128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.05%     802816.0\n",
            "78               layer2.2.aa   128  28  28   128  28  28         0.0       0.38              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "79            layer2.2.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.00%    2269184.0\n",
            "80              layer2.2.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.48%    3215360.0\n",
            "81             layer2.2.act3   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.04%    3211264.0\n",
            "82            layer2.3.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       0.97%    2269184.0\n",
            "83              layer2.3.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.24%     803840.0\n",
            "84             layer2.3.act1   128  28  28   128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.04%     802816.0\n",
            "85            layer2.3.conv2   128  28  28   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       1.02%    1392640.0\n",
            "86              layer2.3.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.22%     803840.0\n",
            "87       layer2.3.drop_block   128  28  28   128  28  28         0.0       0.38              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "88             layer2.3.act2   128  28  28   128  28  28         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.05%     802816.0\n",
            "89               layer2.3.aa   128  28  28   128  28  28         0.0       0.38              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "90            layer2.3.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.04%    2269184.0\n",
            "91              layer2.3.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.46%    3215360.0\n",
            "92             layer2.3.act3   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.04%    3211264.0\n",
            "93            layer3.0.conv1   512  28  28   256  28  28    131072.0       0.77    205,320,192.0    102,760,448.0   2129920.0     802816.0       2.12%    2932736.0\n",
            "94              layer3.0.bn1   256  28  28   256  28  28       512.0       0.77        802,816.0        401,408.0    804864.0     802816.0       0.24%    1607680.0\n",
            "95             layer3.0.act1   256  28  28   256  28  28         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0\n",
            "96            layer3.0.conv2   256  28  28   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   3162112.0     200704.0       1.80%    3362816.0\n",
            "97              layer3.0.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.17%     403456.0\n",
            "98       layer3.0.drop_block   256  14  14   256  14  14         0.0       0.19              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "99             layer3.0.act2   256  14  14   256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.04%     401408.0\n",
            "100              layer3.0.aa   256  14  14   256  14  14         0.0       0.19              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "101           layer3.0.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       1.30%    2052096.0\n",
            "102             layer3.0.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.24%    1613824.0\n",
            "103            layer3.0.act3  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0\n",
            "104    layer3.0.downsample.0   512  28  28   512  14  14         0.0       0.38        401,408.0        401,408.0   1605632.0     401408.0       0.47%    2007040.0\n",
            "105    layer3.0.downsample.1   512  14  14  1024  14  14    524288.0       0.77    205,320,192.0    102,760,448.0   2498560.0     802816.0       1.47%    3301376.0\n",
            "106    layer3.0.downsample.2  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.20%    1613824.0\n",
            "107           layer3.1.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       1.14%    2052096.0\n",
            "108             layer3.1.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.19%     403456.0\n",
            "109            layer3.1.act1   256  14  14   256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.03%     401408.0\n",
            "110           layer3.1.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.45%    2760704.0\n",
            "111             layer3.1.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.19%     403456.0\n",
            "112      layer3.1.drop_block   256  14  14   256  14  14         0.0       0.19              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "113            layer3.1.act2   256  14  14   256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.04%     401408.0\n",
            "114              layer3.1.aa   256  14  14   256  14  14         0.0       0.19              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "115           layer3.1.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.80%    2052096.0\n",
            "116             layer3.1.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.27%    1613824.0\n",
            "117            layer3.1.act3  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0\n",
            "118           layer3.2.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.97%    2052096.0\n",
            "119             layer3.2.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.20%     403456.0\n",
            "120            layer3.2.act1   256  14  14   256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.03%     401408.0\n",
            "121           layer3.2.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.30%    2760704.0\n",
            "122             layer3.2.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.25%     403456.0\n",
            "123      layer3.2.drop_block   256  14  14   256  14  14         0.0       0.19              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "124            layer3.2.act2   256  14  14   256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.05%     401408.0\n",
            "125              layer3.2.aa   256  14  14   256  14  14         0.0       0.19              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "126           layer3.2.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.77%    2052096.0\n",
            "127             layer3.2.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.32%    1613824.0\n",
            "128            layer3.2.act3  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.05%    1605632.0\n",
            "129           layer3.3.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.94%    2052096.0\n",
            "130             layer3.3.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.21%     403456.0\n",
            "131            layer3.3.act1   256  14  14   256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.03%     401408.0\n",
            "132           layer3.3.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.26%    2760704.0\n",
            "133             layer3.3.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.19%     403456.0\n",
            "134      layer3.3.drop_block   256  14  14   256  14  14         0.0       0.19              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "135            layer3.3.act2   256  14  14   256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.05%     401408.0\n",
            "136              layer3.3.aa   256  14  14   256  14  14         0.0       0.19              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "137           layer3.3.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.77%    2052096.0\n",
            "138             layer3.3.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.22%    1613824.0\n",
            "139            layer3.3.act3  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0\n",
            "140           layer3.4.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.95%    2052096.0\n",
            "141             layer3.4.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.21%     403456.0\n",
            "142            layer3.4.act1   256  14  14   256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.03%     401408.0\n",
            "143           layer3.4.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.23%    2760704.0\n",
            "144             layer3.4.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.20%     403456.0\n",
            "145      layer3.4.drop_block   256  14  14   256  14  14         0.0       0.19              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "146            layer3.4.act2   256  14  14   256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.04%     401408.0\n",
            "147              layer3.4.aa   256  14  14   256  14  14         0.0       0.19              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "148           layer3.4.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.77%    2052096.0\n",
            "149             layer3.4.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.23%    1613824.0\n",
            "150            layer3.4.act3  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0\n",
            "151           layer3.5.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.94%    2052096.0\n",
            "152             layer3.5.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.22%     403456.0\n",
            "153            layer3.5.act1   256  14  14   256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.03%     401408.0\n",
            "154           layer3.5.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.23%    2760704.0\n",
            "155             layer3.5.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.19%     403456.0\n",
            "156      layer3.5.drop_block   256  14  14   256  14  14         0.0       0.19              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "157            layer3.5.act2   256  14  14   256  14  14         0.0       0.19         50,176.0         50,176.0    200704.0     200704.0       0.04%     401408.0\n",
            "158              layer3.5.aa   256  14  14   256  14  14         0.0       0.19              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "159           layer3.5.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.77%    2052096.0\n",
            "160             layer3.5.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.24%    1613824.0\n",
            "161            layer3.5.act3  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0\n",
            "162           layer4.0.conv1  1024  14  14   512  14  14    524288.0       0.38    205,420,544.0    102,760,448.0   2899968.0     401408.0       1.88%    3301376.0\n",
            "163             layer4.0.bn1   512  14  14   512  14  14      1024.0       0.38        401,408.0        200,704.0    405504.0     401408.0       0.18%     806912.0\n",
            "164            layer4.0.act1   512  14  14   512  14  14         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.04%     802816.0\n",
            "165           layer4.0.conv2   512  14  14   512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9838592.0     100352.0       2.68%    9938944.0\n",
            "166             layer4.0.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.26%     204800.0\n",
            "167      layer4.0.drop_block   512   7   7   512   7   7         0.0       0.10              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "168            layer4.0.act2   512   7   7   512   7   7         0.0       0.10         25,088.0         25,088.0    100352.0     100352.0       0.04%     200704.0\n",
            "169              layer4.0.aa   512   7   7   512   7   7         0.0       0.10              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "170           layer4.0.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       1.16%    4696064.0\n",
            "171             layer4.0.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       5.17%     819200.0\n",
            "172            layer4.0.act3  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.04%     802816.0\n",
            "173    layer4.0.downsample.0  1024  14  14  1024   7   7         0.0       0.19        200,704.0        200,704.0    802816.0     200704.0       0.27%    1003520.0\n",
            "174    layer4.0.downsample.1  1024   7   7  2048   7   7   2097152.0       0.38    205,420,544.0    102,760,448.0   8589312.0     401408.0       1.47%    8990720.0\n",
            "175    layer4.0.downsample.2  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.21%     819200.0\n",
            "176           layer4.1.conv1  2048   7   7   512   7   7   1048576.0       0.10    102,735,360.0     51,380,224.0   4595712.0     100352.0       0.92%    4696064.0\n",
            "177             layer4.1.bn1   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.21%     204800.0\n",
            "178            layer4.1.act1   512   7   7   512   7   7         0.0       0.10         25,088.0         25,088.0    100352.0     100352.0       0.03%     200704.0\n",
            "179           layer4.1.conv2   512   7   7   512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       1.50%    9637888.0\n",
            "180             layer4.1.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.18%     204800.0\n",
            "181      layer4.1.drop_block   512   7   7   512   7   7         0.0       0.10              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "182            layer4.1.act2   512   7   7   512   7   7         0.0       0.10         25,088.0         25,088.0    100352.0     100352.0       0.04%     200704.0\n",
            "183              layer4.1.aa   512   7   7   512   7   7         0.0       0.10              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "184           layer4.1.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       0.91%    4696064.0\n",
            "185             layer4.1.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.24%     819200.0\n",
            "186            layer4.1.act3  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.04%     802816.0\n",
            "187           layer4.2.conv1  2048   7   7   512   7   7   1048576.0       0.10    102,735,360.0     51,380,224.0   4595712.0     100352.0       0.98%    4696064.0\n",
            "188             layer4.2.bn1   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.24%     204800.0\n",
            "189            layer4.2.act1   512   7   7   512   7   7         0.0       0.10         25,088.0         25,088.0    100352.0     100352.0       0.04%     200704.0\n",
            "190           layer4.2.conv2   512   7   7   512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       1.43%    9637888.0\n",
            "191             layer4.2.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.23%     204800.0\n",
            "192      layer4.2.drop_block   512   7   7   512   7   7         0.0       0.10              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "193            layer4.2.act2   512   7   7   512   7   7         0.0       0.10         25,088.0         25,088.0    100352.0     100352.0       0.04%     200704.0\n",
            "194              layer4.2.aa   512   7   7   512   7   7         0.0       0.10              0.0              0.0         0.0          0.0       0.01%          0.0\n",
            "195           layer4.2.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       0.95%    4696064.0\n",
            "196             layer4.2.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.30%     819200.0\n",
            "197            layer4.2.act3  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.04%     802816.0\n",
            "198      global_pool.flatten  2048   1   1          2048         0.0       0.01              0.0              0.0         0.0          0.0       0.24%          0.0\n",
            "199         global_pool.pool  2048   7   7  2048   1   1         0.0       0.01              0.0              0.0         0.0          0.0       0.32%          0.0\n",
            "200                       fc          2048             3      6147.0       0.00         12,285.0          6,144.0     32780.0         12.0       0.24%      32792.0\n",
            "total                                                     23533411.0     144.05  8,702,738,429.0  4,363,235,840.0     32780.0         12.0     100.00%  373120408.0\n",
            "===================================================================================================================================================================\n",
            "Total params: 23,533,411\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Total memory: 144.05MB\n",
            "Total MAdd: 8.7GMAdd\n",
            "Total Flops: 4.36GFlops\n",
            "Total MemR+W: 355.84MB\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchstat/reporter.py:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(total_df)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See? It wasn't that hard, was it?! üòâ"
      ],
      "metadata": {
        "id": "2cN1EF53Xj9Q"
      }
    }
  ]
}